{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import RDD\n",
    "from math import exp, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName='creditcard_lowlevel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data into rdd\n",
    "raw_rdd = sc.textFile('creditcard.csv')\n",
    "\n",
    "#Remove the csv header row\n",
    "header = raw_rdd.first()\n",
    "\n",
    "rdd_no_header = raw_rdd.filter(lambda row: row != header)\n",
    "\n",
    "def vectorize(row: str):\n",
    "    \"\"\"Vectorize the features and labels.\"\"\"\n",
    "    values = [float(item) for item in row.split(',')]\n",
    "\n",
    "    #One-hot encoding for labels\n",
    "    one_hot = [1 if index == values[-1] else 0 for index in range(2)]\n",
    "\n",
    "    #Insert an additional feature with value 1.0 as bias, then pack the features and encoded label into a LabeledPoint-like object for later use\n",
    "    return (one_hot, [1.0] + values[:-1])\n",
    "\n",
    "rdd = rdd_no_header.map(vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_softmax(x: tuple[float], weights, return_prob = True):\n",
    "    \"\"\"Compute the dot product of input values with each row of the weights matrix, then apply softmax function to get the probability of each class\n",
    "    . Return a vector of probabilities or a single converted class.\"\"\"\n",
    "\n",
    "    #Compute the dot product of x with each row of weights\n",
    "    x_mul_w = [ sum(w * attr_value for w, attr_value in zip(w_row, x)) for w_row in weights]\n",
    "\n",
    "    #Apply e^(value) on each dot product\n",
    "    exp_ret = [exp(v) for v in x_mul_w]\n",
    "\n",
    "    #Sum the e^(dot_product) to get the denominator\n",
    "    sum_exp = sum(exp_ret)\n",
    "\n",
    "    #Divide each e^(dot_product) with sum to get the probability of class\n",
    "    ret = [v / sum_exp for v in exp_ret]\n",
    "\n",
    "    #Return raw vector of probabilities\n",
    "    if return_prob:\n",
    "        return ret\n",
    "    \n",
    "    #Or get the class with highest probability\n",
    "    posit_index = max(ret)\n",
    "    return [1 if value == posit_index else 0 for value in ret]\n",
    "\n",
    "\n",
    "\n",
    "def compute_gradient(x: tuple[float], predicted: tuple[float], true_label: tuple[float]):\n",
    "    \"\"\"Compute the gradient of weights matrix at a specific data point.\"\"\"\n",
    "\n",
    "    #Compute the different between the predicted probability of a class with its true probability\n",
    "    output_diff = [pred - true_lb for pred, true_lb in zip(predicted, true_label)]\n",
    "\n",
    "    #Compute the gradient of weights matrix\n",
    "    this_x__w_gradient = [[diff * attr_value for attr_value in x]  for diff in output_diff]\n",
    "\n",
    "    return this_x__w_gradient\n",
    "\n",
    "\n",
    "\n",
    "def compute_crossEntropy(predicted: tuple[float], true_label: tuple[float]):\n",
    "    \"\"\"Compute the Cross-Entropy loss at a specific data point.\"\"\"\n",
    "\n",
    "    #Apply ln() function on each class probability\n",
    "    predicted_logged = [log(value) for value in predicted]\n",
    "\n",
    "    #Compute the Cross-Entropy value using the vector of natural log applied probabilities and the true_probabilities vector\n",
    "    return -sum(pred_v * true_v for pred_v, true_v in zip(predicted_logged, true_label))\n",
    "\n",
    "\n",
    "\n",
    "def softmax_regression(data: RDD, learning_rate = 1.0, max_epoch = 10, init_weights = list | None):\n",
    "    \"\"\"Run softmax regression on a RDD-based dataset with given learning rate and number of epochs.\"\"\"\n",
    "\n",
    "    #Replicate the weights matrix if given, or else generate a 0 weights matrix\n",
    "    running_weights = []\n",
    "    if init_weights:\n",
    "        running_weights = init_weights.copy()\n",
    "    else:\n",
    "        first_row = data.first()\n",
    "        running_weights = [[0.0] * len(first_row[0])] * len(first_row[1])\n",
    "\n",
    "    #Log for Cross-Entropy loss\n",
    "    cross_entropy_log = []\n",
    "\n",
    "\n",
    "\n",
    "    def gradient_crossEntropy(row: tuple[list[float], list[int]], weights: list[list[float]]):\n",
    "        \"\"\"Wrapper function for computing the gradient of weights matrix and the Cross-Entropy loss at a data point stored in a pyspark.RDD\"\"\"\n",
    "\n",
    "        #Make a prediction\n",
    "        predicted_prob = compute_softmax(row[0], weights)\n",
    "\n",
    "        #Compute the gradient of weights matrix\n",
    "        gradient_w = compute_gradient(row[0], predicted_prob, row[1])\n",
    "\n",
    "        #Compute the Cross-Entropy loss\n",
    "        loss = compute_crossEntropy(predicted_prob, row[1])\n",
    "\n",
    "        return gradient_w, loss\n",
    "    \n",
    "    \n",
    "\n",
    "    def matrix_add(matA: list[list[float]], matB: list[list[float]]):\n",
    "        \"\"\"Function for adding two matrices element-wise.\"\"\"\n",
    "        return [[valueA + valueB for valueA, valueB in zip(rowA, rowB)] for rowA, rowB in zip(matA, matB) ]\n",
    "    \n",
    "\n",
    "\n",
    "    #Number of data points\n",
    "    data_size = data.count()\n",
    "\n",
    "    #Main loop for training\n",
    "    for i in range(max_epoch):\n",
    "        #Predict the probabilites of a data point, then compute the gradient of weights matrix and the Cross-Entropy loss at a data point\n",
    "        rdd_computed = data.map(lambda row: gradient_crossEntropy(row, running_weights))\n",
    "\n",
    "        #Use rdd.reduce to compute the sum of all gradient matrices and the sum of all Cross-Entropy values\n",
    "        gradient_sum, loss_sum = rdd_computed.reduce(lambda row1, row2: (\n",
    "                                                matrix_add(row1[0], row2[0]), #New sum of gradient matrix\n",
    "                                                row1[1] + row2[1] #New sum of Cross-Entropy\n",
    "                                            )\n",
    "                                    )\n",
    "\n",
    "        #The final gradient of weights matrix is the average of all gradient matrix\n",
    "        gradient_w = [[value / data_size for value in row] for row in gradient_sum]\n",
    "\n",
    "        #The final loss value is the average of all Cross-Entropy values\n",
    "        loss = loss_sum / data_size\n",
    "        cross_entropy_log.append(loss)\n",
    "\n",
    "        #Update the weights matrix for next epoch\n",
    "        running_weights = [\n",
    "                            [    old_w - learning_rate * grad_w    for old_w, grad_w in zip(row, row_grad)]\n",
    "                        for row, row_grad in zip(running_weights, gradient_w)\n",
    "                    ]\n",
    "\n",
    "        #Log the weights matrix to console\n",
    "        print(running_weights)\n",
    "\n",
    "    return running_weights, cross_entropy_log\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MRJOB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
