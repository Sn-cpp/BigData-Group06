{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, RDD\n",
    "from pyspark.statcounter import StatCounter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SparkConf()\\\n",
    "            .set(\"spark.driver.memory\", \"4g\")\\\n",
    "            .set(\"spark.executor.memory\", \"8g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/03/31 17:47:12 WARN Utils: Your hostname, DESKTOP-0H87CFM resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/03/31 17:47:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/31 17:47:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(appName='taxi_duration_lowlevel', conf=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Convert data into rdd\n",
    "raw_rdd = sc.textFile('train.csv')\n",
    "\n",
    "#Remove the csv header row\n",
    "header = raw_rdd.first()\n",
    "\n",
    "#Strip the header row\n",
    "rdd_no_header = raw_rdd.filter(lambda row: row != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(row: str):\n",
    "    \"\"\"Vectorize the features and labels.\"\"\"\n",
    "\n",
    "    #Split the string for elements, excluding the first feature (`id`)\n",
    "    values = [item for item in row.split(',')]\n",
    "\n",
    "    #Split the timestamps features into time elements\n",
    "    pickup_time = datetime.strptime(values[2], '%Y-%m-%d %H:%M:%S').strftime('%Y:%m:%d:%H:%M:%S').split(':')\n",
    "    dropoff_time = datetime.strptime(values[3], '%Y-%m-%d %H:%M:%S').strftime('%Y:%m:%d:%H:%M:%S').split(':')\n",
    "    \n",
    "    #Encode the `store_and_fwd_flag` into binary values\n",
    "    values[9] = 1 if values[9] == 'Y' else 0\n",
    "\n",
    "    #Cast strings into number\n",
    "    raw_vector = [float(value) for value in [values[1]] + pickup_time + dropoff_time + values[4:]]\n",
    "    \n",
    "    #Build and return the LabeledPoint-like row\n",
    "    return {'label': raw_vector[-1], 'features' : raw_vector[:-1]}\n",
    "\n",
    "rdd = rdd_no_header.map(vectorize).repartition(8).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth = 1):\n",
    "        self.max_depth = max_depth\n",
    "        self.rules = None\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def set_maxDepth(self, depth):\n",
    "        self.max_depth = depth\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, train_rdd: RDD):\n",
    "        self.num_features = len(rdd.first()['features'])\n",
    "        self.usable_features = [i for i in range(self.num_features)]\n",
    "        self.rules = self.__build_rule_tree_recursive(train_rdd, self.usable_features)\n",
    "\n",
    "    \n",
    "\n",
    "    def transform(self, rdd: RDD):\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def display_rule_tree(self):\n",
    "        def display_tree_recusive(rules: dict, indent = 0):\n",
    "            if not rules:\n",
    "                return \n",
    "            print(f\"{indent * ' '}if features[{rules['split_feature']}] <= {rules['split_point']}\")\n",
    "            display_tree_recusive(rules['left'], indent + 5)\n",
    "            display_tree_recusive(rules['right'], indent + 5)\n",
    "\n",
    "        display_tree_recusive(self.rules)\n",
    "\n",
    "    \n",
    "    def __find_best_split(self, rdd: RDD, usable_features: list): \n",
    "\n",
    "            \n",
    "        def find_feature_best_split(values: list[list[float ] ], parent_info: StatCounter):\n",
    "            sorted_values = sorted(values)\n",
    "            \n",
    "            best_split, best_var_reduction = None, -float(\"inf\")\n",
    "\n",
    "            if any(sorted_values[i][0] != sorted_values[0][0] for i in range(1, parent_info.count())):\n",
    "                parent_pow_sum = parent_info.variance() + parent_info.mean() ** 2\n",
    "\n",
    "                left_sum, left_pow_sum, left_count = 0, 0, 0\n",
    "                \n",
    "                \n",
    "                previous_feature_val =  None\n",
    "                for feature_val, label in sorted_values:\n",
    "                    left_sum += label\n",
    "                    left_pow_sum += label ** 2\n",
    "                    left_count += 1\n",
    "\n",
    "                    if not previous_feature_val:\n",
    "                        previous_feature_val = feature_val\n",
    "                        continue\n",
    "\n",
    "                    right_count = parent_info.count() - left_count\n",
    "\n",
    "                    if not right_count:\n",
    "                        continue\n",
    "\n",
    "                    right_sum = parent_info.sum() - left_sum\n",
    "                    right_pow_sum = parent_pow_sum - left_pow_sum\n",
    "\n",
    "                    \n",
    "                    left_var = left_pow_sum / left_count - (left_sum / left_count)**2 if left_count > 0 else 0\n",
    "                    right_var = right_pow_sum / right_count - (right_sum / right_count)**2 if right_count > 0 else 0\n",
    "\n",
    "                    var_reduction = parent_info.variance() - left_var * (left_count / parent_info.count()) - right_var * (right_count / parent_info.count())\n",
    "\n",
    "                    if var_reduction > best_var_reduction:\n",
    "                        best_split = (previous_feature_val + feature_val) / 2\n",
    "                        best_var_reduction = var_reduction\n",
    "\n",
    "\n",
    "                    previous_feature_val = feature_val\n",
    "                    \n",
    "\n",
    "            return best_var_reduction, best_split\n",
    "\n",
    "        def partition_feature_grouping(partition, usable_features: list):\n",
    "            feature_stats = {}\n",
    "\n",
    "            for row in partition:\n",
    "                for feature in usable_features:\n",
    "                    # Store feature-wise values in a dictionary\n",
    "                    if feature not in feature_stats:\n",
    "                        feature_stats[feature] = []\n",
    "\n",
    "                    feature_stats[feature].append((row['features'][feature], row['label']))\n",
    "\n",
    "            results = []\n",
    "            \n",
    "            for feature, values in feature_stats.items():\n",
    "                results.append((feature, values))\n",
    "        \n",
    "            return iter(results)\n",
    "\n",
    "\n",
    "        parent_info = rdd.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2))\n",
    "        \n",
    "        max_info = rdd.mapPartitions(lambda partition: partition_feature_grouping(partition, usable_features), True)\\\n",
    "                      .reduceByKey(lambda l1, l2: l1 + l2)\\\n",
    "                      .mapValues(lambda values: find_feature_best_split(values, parent_info))\\\n",
    "                      .max(key= lambda tupl: tupl[1][0])\n",
    "\n",
    "        return max_info[0], max_info[1][1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __build_rule_tree_recursive(self, parent: RDD, usable_features: list, depth = 0):\n",
    "        if depth == self.max_depth or parent.isEmpty():\n",
    "            return None\n",
    "    \n",
    "        split_feature, split_point = self.__find_best_split(parent, usable_features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if split_feature == None or split_point == None:\n",
    "            return None\n",
    "\n",
    "\n",
    "        def splitter(iterator, split_feature: int, split_point: float, operand):\n",
    "            for row in iterator:\n",
    "                if operand(row['features'][split_feature], split_point):\n",
    "                    yield row\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        left_rdd = parent.mapPartitions(lambda iterator: splitter(iterator, split_feature, split_point, lambda a,b: a <= b))\n",
    "        right_rdd = parent.mapPartitions(lambda iterator: splitter(iterator, split_feature, split_point, lambda a,b: a > b))\n",
    "        \n",
    "        new_features = [v for v in usable_features if v != split_feature]\n",
    "\n",
    "        return {\n",
    "            'split_feature' : split_feature,\n",
    "            'split_point' : split_point,\n",
    "            'left' : self.__build_rule_tree_recursive(left_rdd, new_features, depth + 1),\n",
    "            'right' : self.__build_rule_tree_recursive(right_rdd, new_features.copy(), depth + 1)\n",
    "        }\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/31 17:47:16 WARN BlockManager: Task 7 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:47:17 WARN BlockManager: Task 8 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:47:34 WARN BlockManager: Task 33 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:47:47 WARN BlockManager: Task 58 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:48:03 WARN BlockManager: Task 83 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:48:15 WARN BlockManager: Task 108 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:48:23 WARN BlockManager: Task 133 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:48:34 WARN BlockManager: Task 158 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:48:42 WARN BlockManager: Task 183 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:48:53 WARN BlockManager: Task 208 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:49:04 WARN BlockManager: Task 233 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:49:12 WARN BlockManager: Task 292 already completed, not releasing lock for rdd_7_2\n",
      "25/03/31 17:49:12 WARN BlockManager: Task 291 already completed, not releasing lock for rdd_7_1\n",
      "25/03/31 17:49:14 WARN BlockManager: Task 321 already completed, not releasing lock for rdd_7_2\n",
      "25/03/31 17:49:15 WARN BlockManager: Task 353 already completed, not releasing lock for rdd_7_5\n",
      "25/03/31 17:49:16 WARN BlockManager: Task 382 already completed, not releasing lock for rdd_7_2\n",
      "25/03/31 17:49:18 WARN BlockManager: Task 410 already completed, not releasing lock for rdd_7_1\n",
      "25/03/31 17:49:19 WARN BlockManager: Task 438 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:49:19 WARN BlockManager: Task 463 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:49:20 WARN BlockManager: Task 488 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:49:21 WARN BlockManager: Task 513 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:49:22 WARN BlockManager: Task 539 already completed, not releasing lock for rdd_7_1\n",
      "25/03/31 17:49:22 WARN BlockManager: Task 542 already completed, not releasing lock for rdd_7_4\n",
      "25/03/31 17:49:22 WARN BlockManager: Task 540 already completed, not releasing lock for rdd_7_2\n",
      "25/03/31 17:49:23 WARN BlockManager: Task 568 already completed, not releasing lock for rdd_7_1\n",
      "25/03/31 17:49:23 WARN BlockManager: Task 571 already completed, not releasing lock for rdd_7_4\n",
      "25/03/31 17:49:25 WARN BlockManager: Task 600 already completed, not releasing lock for rdd_7_4\n",
      "25/03/31 17:49:26 WARN BlockManager: Task 626 already completed, not releasing lock for rdd_7_1\n",
      "25/03/31 17:49:27 WARN BlockManager: Task 656 already completed, not releasing lock for rdd_7_2\n",
      "25/03/31 17:49:28 WARN BlockManager: Task 686 already completed, not releasing lock for rdd_7_3\n",
      "25/03/31 17:49:29 WARN BlockManager: Task 712 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:49:30 WARN BlockManager: Task 737 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:49:31 WARN BlockManager: Task 763 already completed, not releasing lock for rdd_7_1\n",
      "25/03/31 17:49:32 WARN BlockManager: Task 791 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:49:33 WARN BlockManager: Task 816 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:49:34 WARN BlockManager: Task 841 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:49:35 WARN BlockManager: Task 873 already completed, not releasing lock for rdd_7_7\n",
      "25/03/31 17:49:35 WARN BlockManager: Task 871 already completed, not releasing lock for rdd_7_5\n",
      "25/03/31 17:49:35 WARN BlockManager: Task 872 already completed, not releasing lock for rdd_7_6\n",
      "25/03/31 17:49:36 WARN BlockManager: Task 898 already completed, not releasing lock for rdd_7_0\n",
      "25/03/31 17:49:37 WARN BlockManager: Task 926 already completed, not releasing lock for rdd_7_3\n",
      "25/03/31 17:49:38 WARN BlockManager: Task 952 already completed, not releasing lock for rdd_7_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if features[0] <= 2.0\n",
      "     if features[9] <= 31.0\n",
      "          if features[2] <= 6.0\n",
      "               if features[8] <= 6.0\n",
      "                    if features[13] <= 6.0\n",
      "                         if features[4] <= 23.0\n",
      "                              if features[10] <= 23.0\n",
      "                                   if features[3] <= 31.0\n",
      "                                        if features[6] <= 59.0\n",
      "                                             if features[5] <= 59.0\n",
      "                         if features[3] <= 15.5\n",
      "                              if features[14] <= -74.10657501220703\n",
      "                    if features[4] <= 22.0\n",
      "                         if features[14] <= -73.93830490112305\n",
      "                              if features[11] <= 9.0\n",
      "                                   if features[12] <= 52.0\n",
      "                                        if features[13] <= 1.0\n",
      "                                             if features[10] <= 14.5\n",
      "                         if features[6] <= 59.0\n",
      "                              if features[5] <= 3.0\n",
      "                                   if features[13] <= 2.0\n",
      "                                        if features[15] <= 40.77609062194824\n",
      "                                             if features[11] <= 17.5\n",
      "                                             if features[17] <= 40.75842475891113\n",
      "                                        if features[11] <= 22.0\n",
      "                                             if features[16] <= -73.98151016235352\n",
      "                                             if features[16] <= -73.98241806030273\n"
     ]
    }
   ],
   "source": [
    "estimator = DecisionTreeRegressor(10)\n",
    "estimator.fit(rdd)\n",
    "\n",
    "estimator.display_rule_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrjob_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
