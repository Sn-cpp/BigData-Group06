{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor with Pyspark low-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, RDD\n",
    "from pyspark.statcounter import StatCounter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Set the memory usage configurations for Pyspark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the config for spark to boost performance\n",
    "config = SparkConf()\\\n",
    "            .set(\"spark.driver.memory\", \"4g\")\\\n",
    "            .set(\"spark.executor.memory\", \"8g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize a Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/04/05 15:36:10 WARN Utils: Your hostname, DESKTOP-0H87CFM resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/05 15:36:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/05 15:36:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#pyspark init\n",
    "sc = SparkContext(appName='taxi_duration_lowlevel', conf=config).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def read_csv(filepath: str):\n",
    "    \"\"\"Read csv file into a rdd of values and a list of feature columns separately.\"\"\"\n",
    "    #Read the data into rdd\n",
    "    raw_rdd = sc.textFile(filepath)\n",
    "\n",
    "    #Remove the csv header row\n",
    "    header = raw_rdd.first()\n",
    "\n",
    "    #Strip the header row\n",
    "    rdd_no_header = raw_rdd.filter(lambda row: row != header)\n",
    "\n",
    "    return rdd_no_header, header.split(',')\n",
    "\n",
    "\n",
    "raw_train_rdd, train_header = read_csv('train.csv')\n",
    "raw_test_rdd, test_header = read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter usable columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_column(row: str, header: list[str], excluding_features: list[str]):\n",
    "    \"\"\"Extract usable feature columns with given excluding filter.\"\"\"\n",
    "\n",
    "    vector = []\n",
    "\n",
    "    #Split the row with delimiter `,`\n",
    "    values = row.split(',')\n",
    "    \n",
    "    #Filter values\n",
    "    for feature, value in zip(header, values):\n",
    "        if feature not in excluding_features:\n",
    "            vector.append((feature, value))\n",
    "                \n",
    "    return dict(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess features into usable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(row: dict, label_col = None):\n",
    "    \"\"\"- Flatten `pickup_datetime` into separate elements (day, month, year,...).\n",
    "       - Encode `store_and_fwd_flag` to binary values.\n",
    "       - Cast strings to numeric values.\"\"\"\n",
    "\n",
    "    dict_row = {}\n",
    "\n",
    "    #Flatten datetime strings into component elements\n",
    "    row['pickup_datetime'] = row['pickup_datetime'].replace('-', ':').replace(' ', ':').split(':')\n",
    "\n",
    "    #Encode binary values feature\n",
    "    row['store_and_fwd_flag'] = 1 if row['store_and_fwd_flag'] == 'Y' else 0\n",
    "\n",
    "    #Cast strings to float\n",
    "    features = [float(value) for value in [\n",
    "                    row['vendor_id'],\n",
    "                    *row['pickup_datetime'],\n",
    "                    row['passenger_count'],\n",
    "                    row['pickup_longitude'],\n",
    "                    row['pickup_latitude'],\n",
    "                    row['dropoff_longitude'],\n",
    "                    row['dropoff_latitude'],\n",
    "                    row['store_and_fwd_flag']\n",
    "                ]\n",
    "        ]\n",
    "    \n",
    "    #Return a dict of { 'features': vector_of_values, 'label': label (if any) }\n",
    "    dict_row['features'] = features\n",
    "    dict_row['label'] = float(row[label_col]) if label_col else None\n",
    "\n",
    "    return dict_row\n",
    "\n",
    "train_rdd = raw_train_rdd.map(lambda row: extract_column(row, train_header, ['id', 'dropoff_datetime']))\\\n",
    "                         .map(lambda row: preprocess_data(row, 'trip_duration'))\n",
    "\n",
    "    \n",
    "test_rdd = raw_test_rdd.map(lambda row: extract_column(row, test_header, ['id']))\\\n",
    "                       .map(lambda row: preprocess_data(row, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training and testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`partition_feature_grouping` for flattening each row of dataset into a tuple of (feature_value, row_label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_feature_grouping(partition, usable_features: list):\n",
    "    \"\"\"Read each row and convert into a list of (feature_value, row_label), then flatten the results.\"\"\"\n",
    "    feature_stats = {}\n",
    "\n",
    "    for row in partition:\n",
    "        for feature in usable_features:\n",
    "            # Store feature-wise values in a dictionary\n",
    "            if feature not in feature_stats:\n",
    "                feature_stats[feature] = []\n",
    "\n",
    "            feature_stats[feature].append((row['features'][feature], row['label']))\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for feature, values in feature_stats.items():\n",
    "        results.append((feature, values))\n",
    "\n",
    "    return iter(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_split_feature` for finding splitting point with maximum variance reduction on the domain of a feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split_feature(values: list[list[float ] ], parent_info: StatCounter):\n",
    "    \"\"\"Find the splitting point (best variance reduction) of a continuous-values feature using left-right separation, return the mean of 2 values at the border.\"\"\"\n",
    "    \n",
    "    #Sort all values in ascending order\n",
    "    sorted_values = sorted(values)\n",
    "\n",
    "    #Get the number of values\n",
    "    parent_count = parent_info.count()\n",
    "\n",
    "    #Get the E(X^2) of the whole population based on E(X^2) = Var(X) + E^2(X)\n",
    "    parent_pow_sum = parent_info.variance() + parent_info.mean() ** 2\n",
    "    \n",
    "\n",
    "    left_sum, left_pow_sum, left_count = 0, 0, 0\n",
    "    best_split = (-float(\"inf\"), None) \n",
    "\n",
    "\n",
    "    for i in range(0, parent_count - 1):\n",
    "        feature_value, label = sorted_values[i]\n",
    "\n",
    "        #Accumulative sum of y, sum of y^2 and number of values on the left of the currently inspecting splitting point\n",
    "        left_sum += label\n",
    "        left_pow_sum += label ** 2\n",
    "        left_count += 1\n",
    "\n",
    "        #For filtering a sequence of similar values, only consider a valid split between 2 different values\n",
    "        if feature_value == sorted_values[i+1][0]:\n",
    "            continue\n",
    "\n",
    "        #Infer the accumulative results on the right side from the parent and the left information\n",
    "        right_count = parent_count - left_count\n",
    "        right_sum = parent_info.sum() - left_sum\n",
    "        right_pow_sum = parent_pow_sum - left_pow_sum\n",
    "\n",
    "        #Compute variance of the left subset, right subset and the variance reduction using this splitting point\n",
    "        left_var = left_pow_sum / left_count - (left_sum / left_count)**2 if left_count > 0 else 0\n",
    "        \n",
    "        right_var = right_pow_sum / right_count - (right_sum / right_count)**2 if right_count > 0 else 0\n",
    "\n",
    "        var_reduction = parent_info.variance() - left_var * (left_count / parent_count) - right_var * (right_count / parent_count)     \n",
    "\n",
    "        #Update the best splitting point information         \n",
    "        if var_reduction > best_split[0]:\n",
    "            best_split = ( var_reduction, (feature_value + sorted_values[i+1][0]) / 2 )\n",
    "\n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-function `splitter` for splitting the rows of the parent dataset according to the splitting point and operands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(iterator, split_feature: int, split_point: float, operand):\n",
    "    ret = []\n",
    "    for row in iterator:\n",
    "        if operand(row['features'][split_feature], split_point):\n",
    "            ret.append(row)\n",
    "\n",
    "    return iter(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main class `DecisionTreeRegressor` for building and executing Decision Tree Regressor Algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth = 1):\n",
    "        #Initialize the estimator with given depth (if any)\n",
    "        self.max_depth = max_depth\n",
    "        self.rules = None\n",
    "        pass\n",
    "\n",
    "    def set_maxDepth(self, depth):\n",
    "        \"\"\"Set current maximum depth for the estimator.\"\"\"\n",
    "        self.max_depth = depth\n",
    "\n",
    "    def fit(self, train_rdd: RDD):\n",
    "        \"\"\"Execute Decision Tree Algorithm recursively on a given rdd based on variance reduction and the current maximum depth.\"\"\"\n",
    "        self.num_features = len(train_rdd.first()['features'])\n",
    "        self.usable_features = [i for i in range(self.num_features)]\n",
    "        sample_size = train_rdd.count()\n",
    "        \n",
    "        return self.__build_rule_tree_recursive(train_rdd, self.usable_features, sample_size)\n",
    "\n",
    "    def transform(self, rdd: RDD):\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def display_rule_tree(self, model: dict):\n",
    "        \"\"\"Recursively display the rules of a Decision Tree model.\"\"\"\n",
    "        self.__display_tree_recusive(model)\n",
    "\n",
    "\n",
    "\n",
    "    def __display_tree_recusive(self, rules: dict, indent = 0):\n",
    "        #Stopping condition\n",
    "        if not rules:\n",
    "            return\n",
    "\n",
    "        if len(rules) == 1:\n",
    "            #Is a leaf condition\n",
    "            print(f\"{indent * ' '}Predict: {rules['prediction']}\")\n",
    "            return\n",
    "\n",
    "        #Print the splitting point information and call recursion of the left and right child\n",
    "\n",
    "        print(f\"{indent * ' '}If  features[{rules['split_feature']}] <= {rules['split_point']}\")\n",
    "        self.__display_tree_recusive(rules['left'], indent + 5)\n",
    "        print(f\"{indent * ' '}Else  features[{rules['split_feature']}] > {rules['split_point']}\")\n",
    "        self.__display_tree_recusive(rules['right'], indent + 5)         \n",
    "\n",
    "    def __find_best_split(self, rdd: RDD, usable_features: list):         \n",
    "        \"\"\"Find the best splitting point with maximum variance reduction of the input dataset.\"\"\"\n",
    "\n",
    "        #Compute the dataset statistics and store into a StatCounter object\n",
    "        parent_info = rdd.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2))\n",
    "        \n",
    "        #Convert each row into a list of (row_feature_value, row_label) and flatten the results\n",
    "        processed_rdd = rdd.mapPartitions(lambda partition: partition_feature_grouping(partition, usable_features), True)\\\n",
    "                      .reduceByKey(lambda l1, l2: l1 + l2).cache()\n",
    "        \n",
    "        #Find the best splitting point for each features\n",
    "        best_split_per_feature = processed_rdd.mapValues(lambda values: find_split_feature(values, parent_info))\\\n",
    "                                  \n",
    "        #Find the best splitting point for the dataset\n",
    "        best_split = max(best_split_per_feature.collect(),key=lambda x: (x[1][0], -x[0]))\n",
    "\n",
    "        return best_split[0], best_split[1][1]\n",
    "\n",
    "\n",
    "    def __build_rule_tree_recursive(self, parent: RDD, usable_features: list, sample_size, depth = 0):\n",
    "        \"\"\"Recursively build the decision tree by finding the splitting point with maximum variance reduction and split the dataset with this point, up to the maximum depth.\"\"\"\n",
    "        \n",
    "        #Stopping condition\n",
    "        if sample_size == 0:\n",
    "            return None\n",
    "        \n",
    "        #Return the mean of un-splitted subset as the prediction if reached the depth limit\n",
    "        if depth == self.max_depth:\n",
    "            mean = parent.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2)).mean()\n",
    "\n",
    "            return {'prediction' : mean}\n",
    "\n",
    "        #Find the best splitting point for the input dataset\n",
    "        split_feature, split_point = self.__find_best_split(parent, usable_features)\n",
    "        \n",
    "        #Return the mean of un-splitted subset as the prediction if no splitting point is valid but has not reached the depth limit yet\n",
    "        if split_point == None:\n",
    "            mean = parent.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2)).mean()\n",
    "            \n",
    "            return {'prediction' : mean}\n",
    "\n",
    "        #Split the dataset into left and right subsets\n",
    "        left_rdd = parent.mapPartitions(lambda iterator: splitter(iterator, split_feature, split_point, lambda a,b: a <= b), True).cache()\n",
    "        right_rdd = parent.mapPartitions(lambda iterator: splitter(iterator, split_feature, split_point, lambda a,b: a >= b), True).cache()\n",
    "        \n",
    "        #Un-cache the parent dataset (excluding the input one)\n",
    "        if depth > 0:\n",
    "            parent.unpersist()\n",
    "\n",
    "        #Compute the size of the left subset\n",
    "        left_sample_size = left_rdd.count()\n",
    "     \n",
    "        #Build the dict of information with recursion call\n",
    "        return {\n",
    "            'split_feature' : split_feature,\n",
    "            'split_point' : split_point,\n",
    "            'left' : self.__build_rule_tree_recursive(left_rdd, usable_features, left_sample_size, depth + 1),\n",
    "            'right' : self.__build_rule_tree_recursive(right_rdd, usable_features, sample_size - left_sample_size, depth + 1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 157:>                                                        (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If  features[8] <= -73.88602066040039\n",
      "     If  features[10] <= -73.91103744506836\n",
      "          If  features[11] <= 40.70149040222168\n",
      "               If  features[9] <= 40.725751876831055\n",
      "                    If  features[11] <= 40.701486587524414\n",
      "                         Predict: 1019.1713947990531\n",
      "                    Else  features[11] > 40.701486587524414\n",
      "                         Predict: 28934.666666666668\n",
      "               Else  features[9] > 40.725751876831055\n",
      "                    If  features[9] <= 40.72575569152832\n",
      "                         Predict: 22868.0\n",
      "                    Else  features[9] > 40.72575569152832\n",
      "                         Predict: 1901.4971375584564\n",
      "          Else  features[11] > 40.70149040222168\n",
      "               If  features[0] <= 1.5\n",
      "                    If  features[8] <= -73.92168045043945\n",
      "                         Predict: 712.2167951813674\n",
      "                    Else  features[8] > -73.92168045043945\n",
      "                         Predict: 2017.9384793964005\n",
      "               Else  features[0] > 1.5\n",
      "                    If  features[11] <= 41.24163627624512\n",
      "                         Predict: 920.4392529340327\n",
      "                    Else  features[11] > 41.24163627624512\n",
      "                         Predict: 85561.0\n",
      "     Else  features[10] > -73.91103744506836\n",
      "          If  features[10] <= -73.81863784790039\n",
      "               If  features[8] <= -73.95466995239258\n",
      "                    If  features[4] <= 12.5\n",
      "                         Predict: 1731.4917981072565\n",
      "                    Else  features[4] > 12.5\n",
      "                         Predict: 2166.5322839459027\n",
      "               Else  features[8] > -73.95466995239258\n",
      "                    If  features[8] <= -73.92405319213867\n",
      "                         Predict: 1275.0604395604396\n",
      "                    Else  features[8] > -73.92405319213867\n",
      "                         Predict: 672.9857090509345\n",
      "          Else  features[10] > -73.81863784790039\n",
      "               If  features[10] <= -73.818603515625\n",
      "                    Predict: 86171.0\n",
      "               Else  features[10] > -73.818603515625\n",
      "                    If  features[4] <= 12.5\n",
      "                         Predict: 2364.122712218931\n",
      "                    Else  features[4] > 12.5\n",
      "                         Predict: 3418.537453716891\n",
      "Else  features[8] > -73.88602066040039\n",
      "     If  features[10] <= -73.95637893676758\n",
      "          If  features[9] <= 40.682716369628906\n",
      "               If  features[10] <= -73.95681762695312\n",
      "                    If  features[9] <= 40.648630142211914\n",
      "                         Predict: 3080.9053322395407\n",
      "                    Else  features[9] > 40.648630142211914\n",
      "                         Predict: 5649.662559890484\n",
      "               Else  features[10] > -73.95681762695312\n",
      "                    If  features[10] <= -73.95680618286133\n",
      "                         Predict: 647897.6666666666\n",
      "                    Else  features[10] > -73.95680618286133\n",
      "                         Predict: 3050.9799999999996\n",
      "          Else  features[9] > 40.682716369628906\n",
      "               If  features[4] <= 18.5\n",
      "                    If  features[4] <= 6.5\n",
      "                         Predict: 1428.209934395501\n",
      "                    Else  features[4] > 6.5\n",
      "                         Predict: 2443.4453028081653\n",
      "               Else  features[4] > 18.5\n",
      "                    If  features[10] <= -73.98086166381836\n",
      "                         Predict: 1834.5476923076926\n",
      "                    Else  features[10] > -73.98086166381836\n",
      "                         Predict: 1468.713033644331\n",
      "     Else  features[10] > -73.95637893676758\n",
      "          If  features[10] <= -73.90579223632812\n",
      "               If  features[8] <= -73.81616592407227\n",
      "                    If  features[11] <= 40.69793128967285\n",
      "                         Predict: 2139.394014962593\n",
      "                    Else  features[11] > 40.69793128967285\n",
      "                         Predict: 1299.9743243243236\n",
      "               Else  features[8] > -73.81616592407227\n",
      "                    If  features[4] <= 18.5\n",
      "                         Predict: 2558.3221438645974\n",
      "                    Else  features[4] > 18.5\n",
      "                         Predict: 2021.9153412006158\n",
      "          Else  features[10] > -73.90579223632812\n",
      "               If  features[7] <= 0.5\n",
      "                    If  features[0] <= 1.5\n",
      "                         Predict: 5.75\n",
      "                    Else  features[0] > 1.5\n",
      "                         Predict: 85901.0\n",
      "               Else  features[7] > 0.5\n",
      "                    If  features[11] <= 40.779428482055664\n",
      "                         Predict: 1097.090286105876\n",
      "                    Else  features[11] > 40.779428482055664\n",
      "                         Predict: 1876.4273858921163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "estimator = DecisionTreeRegressor(5)\n",
    "model = estimator.fit(train_rdd)\n",
    "estimator.display_rule_tree(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrjob_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
