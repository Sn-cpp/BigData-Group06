{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor with Pyspark low-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, RDD\n",
    "from pyspark.statcounter import StatCounter\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Set the memory usage configurations for Pyspark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the config for spark to boost performance\n",
    "config = SparkConf()\\\n",
    "            .set(\"spark.driver.memory\", \"8g\")\\\n",
    "            .set(\"spark.executor.memory\", \"8g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize a Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/04/11 22:27:26 WARN Utils: Your hostname, HP-Envy resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/11 22:27:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/11 22:27:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 49294)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/hatake/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/hatake/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/home/hatake/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/hatake/.local/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#pyspark init\n",
    "sc = SparkContext(appName='taxi_duration_lowlevel', conf=config).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def read_csv(filepath: str):\n",
    "    \"\"\"Read csv file into a rdd of values and a list of feature columns separately.\"\"\"\n",
    "    #Read the data into rdd\n",
    "    raw_rdd = sc.textFile(filepath)\n",
    "\n",
    "    #Remove the csv header row\n",
    "    header = raw_rdd.first()\n",
    "\n",
    "    #Strip the header row\n",
    "    rdd_no_header = raw_rdd.filter(lambda row: row != header)\n",
    "\n",
    "    return rdd_no_header, header.split(',')\n",
    "\n",
    "\n",
    "raw_train_rdd, train_header = read_csv('train.csv')\n",
    "raw_test_rdd, test_header = read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter usable columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_column(row: str, header: list[str], excluding_features: list[str]):\n",
    "    \"\"\"Extract usable feature columns with given excluding filter.\"\"\"\n",
    "    #Split the row with delimiter `,`\n",
    "    values = row.split(',')\n",
    "    \n",
    "    #Filter values\n",
    "    return dict((header[i], values[i]) for i in range(len(header)) if header[i] not in excluding_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess features into usable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_data(row: dict, label_col = None):\n",
    "    \"\"\"- Flatten `pickup_datetime` into separate elements (day, month, year,...).\n",
    "       - Encode `store_and_fwd_flag` to binary values.\n",
    "       - Cast strings to numeric values.\n",
    "       - Add Haversine distance.\"\"\"\n",
    "\n",
    "    #dict_row = {}\n",
    "\n",
    "    #Flatten datetime strings into component elements\n",
    "    #row['pickup_datetime'] = row['pickup_datetime'].replace('-', ':').replace(' ', ':').split(':')\n",
    "    dt = datetime.strptime(row['pickup_datetime'], '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    #Encode binary values feature\n",
    "    row['store_and_fwd_flag'] = 1 if row['store_and_fwd_flag'] == 'Y' else 0\n",
    "    \n",
    "    # Add Haversine distance\n",
    "    R = 6371\n",
    "    lon1, lat1 = math.radians(float(row['pickup_longitude'])), math.radians(float(row['pickup_latitude']))\n",
    "    lon2, lat2 = math.radians(float(row['dropoff_longitude'])), math.radians(float(row['dropoff_latitude']))\n",
    "    dlon, dlat = lon2 - lon1, lat2 - lat1\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    distance_km = R * c\n",
    "    \n",
    "    #Cast strings to float\n",
    "    features = [float(value) for value in [\n",
    "                    float(row['vendor_id']),\n",
    "                    float(dt.year), float(dt.month), float(dt.day), float(dt.hour),\n",
    "                    float(dt.minute), float(dt.second),\n",
    "                    float(row['passenger_count']),\n",
    "                    float(row['pickup_longitude']), \n",
    "                    float(row['pickup_latitude']),\n",
    "                    float(row['dropoff_longitude']), \n",
    "                    float(row['dropoff_latitude']),\n",
    "                    row['store_and_fwd_flag'],\n",
    "                    distance_km\n",
    "                ]\n",
    "        ]\n",
    "    \n",
    "    #Return a dict of { 'features': vector_of_values, 'label': label (if any) }\n",
    "    # dict_row['features'] = features\n",
    "    # dict_row['label'] = float(row[label_col]) if label_col else None\n",
    "\n",
    "    return {'id': row.get('id'), 'features': features, 'label': float(row[label_col]) if label_col else None}\n",
    "\n",
    "# train_rdd = raw_train_rdd.map(lambda row: extract_column(row, train_header, ['id', 'dropoff_datetime']))\\\n",
    "#                          .map(lambda row: preprocess_data(row, 'trip_duration'))\n",
    "\n",
    "    \n",
    "# test_rdd = raw_test_rdd.map(lambda row: extract_column(row, test_header, ['id']))\\\n",
    "#                        .map(lambda row: preprocess_data(row, None))                 \n",
    "train_rdd = raw_train_rdd.map(lambda row: extract_column(row, train_header, ['dropoff_datetime'])) \\\n",
    "                         .map(lambda row: preprocess_data(row, 'trip_duration'))\n",
    "                         \n",
    "test_rdd = raw_test_rdd.map(lambda row: extract_column(row, test_header, [])) \\\n",
    "                       .map(lambda row: preprocess_data(row))\n",
    "                       \n",
    "# Cache to reuse\n",
    "train_rdd.cache()\n",
    "test_rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`partition_feature_grouping` for flattening each row of dataset into a tuple of (feature_value, row_label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_feature_grouping(partition, usable_features: list):\n",
    "    \"\"\"Read each row and convert into a list of (feature_value, row_label), then flatten the results.\"\"\"\n",
    "    feature_stats = {}\n",
    "    for row in partition:\n",
    "        for feature in usable_features:\n",
    "            # Store feature-wise values in a dictionary\n",
    "            if feature not in feature_stats:\n",
    "                feature_stats[feature] = []\n",
    "\n",
    "            feature_stats[feature].append((row['features'][feature], row['label']))\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for feature, values in feature_stats.items():\n",
    "        results.append((feature, values))\n",
    "\n",
    "    return iter(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_split_feature` for finding splitting point with maximum variance reduction on the domain of a feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split_feature(values: list, parent_info: StatCounter):\n",
    "    # Sort values by feature value (ascending order)\n",
    "    sorted_values = sorted(values, key=lambda x: x[0])\n",
    "\n",
    "    # Get total number of data points\n",
    "    parent_count = parent_info.count()\n",
    "\n",
    "    # Get variance and mean of the whole dataset\n",
    "    parent_var = parent_info.variance()\n",
    "    parent_mean = parent_info.mean()\n",
    "\n",
    "    # Calculate parent's sum of squares: E[X²] = Var(X) + (E[X])²\n",
    "    parent_pow_sum = parent_var + parent_mean ** 2\n",
    "\n",
    "    # If not enough data to split, return no split\n",
    "    if parent_count < 2:\n",
    "        return (-float(\"inf\"), None)\n",
    "\n",
    "    # Initialize variables for left partition\n",
    "    left_sum, left_pow_sum, left_count = 0, 0, 0\n",
    "    best_split = (-float(\"inf\"), None)\n",
    "\n",
    "    # Iterate through possible split points\n",
    "    for i in range(parent_count - 1):\n",
    "        val, label = sorted_values[i]\n",
    "\n",
    "        # Accumulate stats for left partition\n",
    "        left_sum += label\n",
    "        left_pow_sum += label ** 2\n",
    "        left_count += 1\n",
    "\n",
    "        # Skip splitting between identical feature values\n",
    "        if val == sorted_values[i + 1][0]:\n",
    "            continue\n",
    "\n",
    "        # Compute right partition stats by subtracting left from parent\n",
    "        right_count = parent_count - left_count\n",
    "        right_sum = parent_info.sum() - left_sum\n",
    "        right_pow_sum = parent_pow_sum * parent_count - left_pow_sum  # Scale to total sum of squares\n",
    "\n",
    "        # Compute variance for each partition\n",
    "        left_var = (left_pow_sum / left_count - (left_sum / left_count) ** 2) if left_count > 0 else 0\n",
    "        right_var = (right_pow_sum / right_count - (right_sum / right_count) ** 2) if right_count > 0 else 0\n",
    "\n",
    "        # Calculate weighted variance reduction\n",
    "        var_reduction = parent_var - (left_var * left_count + right_var * right_count) / parent_count\n",
    "\n",
    "        # Update best split if this reduces variance more\n",
    "        if var_reduction > best_split[0]:\n",
    "            best_split = (var_reduction, (val + sorted_values[i + 1][0]) / 2)\n",
    "\n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-function `splitter` for splitting the rows of the parent dataset according to the splitting point and operands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(iterator, split_feature: int, split_point: float, operand):\n",
    "    ret = []\n",
    "    for row in iterator:\n",
    "        if operand(row['features'][split_feature], split_point):\n",
    "            ret.append(row)\n",
    "\n",
    "    return iter(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main class `DecisionTreeRegressor` for building and executing Decision Tree Regressor Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synchronized with maxDepth=10 from sections 3.2.1 and 3.2.2 (other parameters like maxBins and minInstancesPerNode cannot be directly applied in manual implementation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth = 1): # Synchronized with maxDepth=10 from sections 3.2.1 and 3.2.2\n",
    "        #Initialize the estimator with given depth (if any)\n",
    "        self.max_depth = max_depth\n",
    "        self.rules = None\n",
    "        self.num_features = None\n",
    "\n",
    "    def set_maxDepth(self, depth):\n",
    "        \"\"\"Set current maximum depth for the estimator.\"\"\"\n",
    "        self.max_depth = depth\n",
    "\n",
    "    def fit(self, train_rdd: RDD):\n",
    "        \"\"\"Execute Decision Tree Algorithm recursively on a given rdd based on variance reduction and the current maximum depth.\"\"\"\n",
    "        self.num_features = len(train_rdd.first()['features'])\n",
    "        self.usable_features = [i for i in range(self.num_features)]\n",
    "        sample_size = train_rdd.count()\n",
    "        print(f\"Starting tree construction with {sample_size} samples, max_depth={self.max_depth}\")\n",
    "        self.rules = self.__build_rule_tree_recursive(train_rdd, self.usable_features, sample_size)\n",
    "        print(\"Tree construction completed\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, rdd: RDD):\n",
    "        \"\"\"Make predictions on an RDD using the decision tree rules.\"\"\"\n",
    "        def predict_row(row):\n",
    "            # Start from the root of the decision tree\n",
    "            node = self.rules\n",
    "\n",
    "            # Traverse the tree until reaching a leaf node (which contains a prediction)\n",
    "            while 'prediction' not in node:\n",
    "                # Compare the feature value with the split point to decide the direction\n",
    "                if row['features'][node['split_feature']] <= node['split_point']:\n",
    "                    node = node['left']  # Go to the left subtree\n",
    "                else:\n",
    "                    node = node['right']  # Go to the right subtree\n",
    "\n",
    "            # Return a tuple of (row ID, predicted value)\n",
    "            return (row['id'], node['prediction'])\n",
    "\n",
    "        # Apply prediction to each row in the RDD\n",
    "        return rdd.map(lambda row: predict_row(row))\n",
    "\n",
    "    def display_rule_tree(self):\n",
    "        \"\"\"Recursively display the rules of a Decision Tree model.\"\"\"\n",
    "        print(\"\\nDecision Tree Rules:\")\n",
    "        self.__display_tree_recursive(self.rules)\n",
    "\n",
    "    def __display_tree_recursive(self, rules: dict, indent = 0):\n",
    "        #Stopping condition\n",
    "        if not rules:\n",
    "            return\n",
    "\n",
    "        if 'prediction' in rules:\n",
    "            #Is a leaf condition\n",
    "            print(f\"{indent * ' '}Predict: {rules['prediction']:.2f}\")\n",
    "            return\n",
    "\n",
    "        #Print the splitting point information and call recursion of the left and right child\n",
    "\n",
    "        print(f\"{indent * ' '}If feature[{rules['split_feature']}] <= {rules['split_point']:.2f}\")\n",
    "        self.__display_tree_recursive(rules['left'], indent + 4)\n",
    "        print(f\"{indent * ' '}Else feature[{rules['split_feature']}] > {rules['split_point']:.2f}\")\n",
    "        self.__display_tree_recursive(rules['right'], indent + 4)       \n",
    "\n",
    "    def __find_best_split(self, rdd: RDD, usable_features: list):         \n",
    "        \"\"\"Find the best splitting point with maximum variance reduction of the input dataset.\"\"\"\n",
    "\n",
    "        #Compute the dataset statistics and store into a StatCounter object\n",
    "        parent_info = rdd.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2))\n",
    "        \n",
    "        #Convert each row into a list of (row_feature_value, row_label) and flatten the results\n",
    "        processed_rdd = rdd.mapPartitions(lambda partition: partition_feature_grouping(partition, usable_features), True)\\\n",
    "                      .reduceByKey(lambda l1, l2: l1 + l2).cache()\n",
    "        \n",
    "        #Find the best splitting point for each features\n",
    "        best_split_per_feature = processed_rdd.mapValues(lambda values: find_split_feature(values, parent_info))\\\n",
    "                                  \n",
    "        #Find the best splitting point for the dataset\n",
    "        best_split = max(best_split_per_feature.collect(),key=lambda x: (x[1][0], -x[0]))\n",
    "\n",
    "        return best_split[0], best_split[1][1]\n",
    "\n",
    "\n",
    "    def __build_rule_tree_recursive(self, parent: RDD, usable_features: list, sample_size, depth = 0):\n",
    "        \"\"\"Recursively build the decision tree by finding the splitting point with maximum variance reduction and split the dataset with this point, up to the maximum depth.\"\"\"\n",
    "        print(f\"Building tree at depth {depth}, sample size: {sample_size}\")\n",
    "        #Stopping condition\n",
    "        if sample_size == 0:\n",
    "            return None\n",
    "        \n",
    "        #Return the mean of un-splitted subset as the prediction if reached the depth limit\n",
    "        if depth == self.max_depth or sample_size < 2:\n",
    "            mean = parent.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2)).mean()\n",
    "\n",
    "            return {'prediction' : mean}\n",
    "\n",
    "        #Find the best splitting point for the input dataset\n",
    "        split_feature, split_point = self.__find_best_split(parent, usable_features)\n",
    "        \n",
    "        #Return the mean of un-splitted subset as the prediction if no splitting point is valid but has not reached the depth limit yet\n",
    "        if split_point == None:\n",
    "            mean = parent.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2)).mean()\n",
    "            \n",
    "            return {'prediction' : mean}\n",
    "\n",
    "        #Split the dataset into left and right subsets\n",
    "        left_rdd = parent.mapPartitions(lambda iterator: splitter(iterator, split_feature, split_point, lambda a,b: a <= b), True).cache()\n",
    "        right_rdd = parent.mapPartitions(lambda iterator: splitter(iterator, split_feature, split_point, lambda a,b: a >= b), True).cache()\n",
    "        \n",
    "        #Un-cache the parent dataset (excluding the input one)\n",
    "        if depth > 0:\n",
    "            parent.unpersist()\n",
    "\n",
    "        #Compute the size of the left subset\n",
    "        left_sample_size = left_rdd.count()\n",
    "     \n",
    "        #Build the dict of information with recursion call\n",
    "        return {\n",
    "            'split_feature' : split_feature,\n",
    "            'split_point' : split_point,\n",
    "            'left' : self.__build_rule_tree_recursive(left_rdd, usable_features, left_sample_size, depth + 1),\n",
    "            'right' : self.__build_rule_tree_recursive(right_rdd, usable_features, sample_size - left_sample_size, depth + 1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 22:27:39 WARN BlockManager: Task 2 already completed, not releasing lock for rdd_6_0\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tree construction with 1458644 samples, max_depth=10\n",
      "Building tree at depth 0, sample size: 1458644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 1, sample size: 1160867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 2, sample size: 677778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 3, sample size: 356233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 166340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 90855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.                         (0 + 6) / 6]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hatake/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hatake/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/hatake/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hatake/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hatake/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/hatake/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training with max_depth=10\u001b[39;00m\n\u001b[1;32m      2\u001b[0m estimator \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# Synchronized with maxDepth=10 from sections 3.2.1 and 3.2.2\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_rdd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m estimator\u001b[38;5;241m.\u001b[39mdisplay_rule_tree()\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, train_rdd)\u001b[0m\n\u001b[1;32m     16\u001b[0m sample_size \u001b[38;5;241m=\u001b[39m train_rdd\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting tree construction with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples, max_depth=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__build_rule_tree_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_rdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musable_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTree construction completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[10], line 123\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.__build_rule_tree_recursive\u001b[0;34m(self, parent, usable_features, sample_size, depth)\u001b[0m\n\u001b[1;32m    117\u001b[0m left_sample_size \u001b[38;5;241m=\u001b[39m left_rdd\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#Build the dict of information with recursion call\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_feature\u001b[39m\u001b[38;5;124m'\u001b[39m : split_feature,\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_point\u001b[39m\u001b[38;5;124m'\u001b[39m : split_point,\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__build_rule_tree_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_rdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musable_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_sample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(right_rdd, usable_features, sample_size \u001b[38;5;241m-\u001b[39m left_sample_size, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    125\u001b[0m }\n",
      "Cell \u001b[0;32mIn[10], line 123\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.__build_rule_tree_recursive\u001b[0;34m(self, parent, usable_features, sample_size, depth)\u001b[0m\n\u001b[1;32m    117\u001b[0m left_sample_size \u001b[38;5;241m=\u001b[39m left_rdd\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#Build the dict of information with recursion call\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_feature\u001b[39m\u001b[38;5;124m'\u001b[39m : split_feature,\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_point\u001b[39m\u001b[38;5;124m'\u001b[39m : split_point,\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__build_rule_tree_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_rdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musable_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_sample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(right_rdd, usable_features, sample_size \u001b[38;5;241m-\u001b[39m left_sample_size, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    125\u001b[0m }\n",
      "    \u001b[0;31m[... skipping similar frames: DecisionTreeRegressor.__build_rule_tree_recursive at line 123 (2 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 123\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.__build_rule_tree_recursive\u001b[0;34m(self, parent, usable_features, sample_size, depth)\u001b[0m\n\u001b[1;32m    117\u001b[0m left_sample_size \u001b[38;5;241m=\u001b[39m left_rdd\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#Build the dict of information with recursion call\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_feature\u001b[39m\u001b[38;5;124m'\u001b[39m : split_feature,\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_point\u001b[39m\u001b[38;5;124m'\u001b[39m : split_point,\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__build_rule_tree_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_rdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musable_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_sample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(right_rdd, usable_features, sample_size \u001b[38;5;241m-\u001b[39m left_sample_size, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    125\u001b[0m }\n",
      "Cell \u001b[0;32mIn[10], line 117\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.__build_rule_tree_recursive\u001b[0;34m(self, parent, usable_features, sample_size, depth)\u001b[0m\n\u001b[1;32m    114\u001b[0m     parent\u001b[38;5;241m.\u001b[39munpersist()\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m#Compute the size of the left subset\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m left_sample_size \u001b[38;5;241m=\u001b[39m \u001b[43mleft_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#Build the dict of information with recursion call\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_feature\u001b[39m\u001b[38;5;124m'\u001b[39m : split_feature,\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_point\u001b[39m\u001b[38;5;124m'\u001b[39m : split_point,\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(left_rdd, usable_features, left_sample_size, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(right_rdd, usable_features, sample_size \u001b[38;5;241m-\u001b[39m left_sample_size, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    125\u001b[0m }\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/rdd.py:2316\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   2296\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2297\u001b[0m \u001b[38;5;124;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   2298\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2314\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/rdd.py:2291\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msum\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[NumberOrArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumberOrArray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2272\u001b[0m \u001b[38;5;124;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   2273\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2289\u001b[0m \u001b[38;5;124;03m    6.0\u001b[39;00m\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   2292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\n\u001b[1;32m   2293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/rdd.py:2044\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   2042\u001b[0m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> 2044\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/rdd.py:1831\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T]:\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;124;03m    Return a list that contains all the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1806\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;124;03m    ['x', 'y', 'z']\u001b[39;00m\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1831\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m         sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mcollectAndServe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd\u001b[38;5;241m.\u001b[39mrdd())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/traceback_utils.py:81\u001b[0m, in \u001b[0;36mSCCallSiteSync.__exit__\u001b[0;34m(self, type, value, tb)\u001b[0m\n\u001b[1;32m     79\u001b[0m SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetCallSite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Training with max_depth=10\n",
    "estimator = DecisionTreeRegressor(max_depth=10)  # Synchronized with maxDepth=10 from sections 3.2.1 and 3.2.2\n",
    "model = estimator.fit(train_rdd)\n",
    "estimator.display_rule_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample predictions for a few test cases (test file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 22:25:39 WARN BlockManager: Task 45 already completed, not releasing lock for rdd_7_0\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: id3004672, Prediction: 735.6992713204887\n",
      "ID: id3505355, Prediction: 735.6992713204887\n",
      "ID: id1217141, Prediction: 735.6992713204887\n",
      "ID: id2150126, Prediction: 1831.9367882677282\n",
      "ID: id1598245, Prediction: 735.6992713204887\n"
     ]
    }
   ],
   "source": [
    "# Predict and display samples\n",
    "predictions_rdd = estimator.transform(test_rdd)\n",
    "print(\"Sample Predictions:\")\n",
    "predictions_samples = predictions_rdd.take(5)\n",
    "for pred in predictions_samples:\n",
    "    print(f\"ID: {pred[0]}, Prediction: {pred[1]}\")\n",
    "\n",
    "# Stop SparkContext\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
