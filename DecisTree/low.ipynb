{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor with Pyspark low-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, RDD\n",
    "from pyspark.statcounter import StatCounter\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Set the memory usage configurations for Pyspark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the config for spark to enhance performance\n",
    "config = SparkConf()\\\n",
    "            .set(\"spark.driver.memory\", \"4g\")\\\n",
    "            .set(\"spark.executor.memory\", \"4g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize a Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/04/12 09:26:38 WARN Utils: Your hostname, HP-Envy resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/12 09:26:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/12 09:26:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#pyspark init\n",
    "sc = SparkContext(appName='taxi_duration_lowlevel', conf=config).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def read_csv(filepath: str):\n",
    "    \"\"\"Read csv file into a rdd of values and a list of feature columns separately.\"\"\"\n",
    "    #Read the data into rdd\n",
    "    raw_rdd = sc.textFile(filepath)\n",
    "\n",
    "    #Remove the csv header row\n",
    "    header = raw_rdd.first()\n",
    "\n",
    "    #Strip the header row\n",
    "    rdd_no_header = raw_rdd.filter(lambda row: row != header)\n",
    "\n",
    "    return rdd_no_header, header.split(',')\n",
    "\n",
    "\n",
    "raw_train_rdd, train_header = read_csv('train.csv')\n",
    "raw_test_rdd, test_header = read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter usable columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_column(row: str, header: list[str], excluding_features: list[str]):\n",
    "    \"\"\"Extract usable feature columns with given excluding filter.\"\"\"\n",
    "    #Split the row with delimiter `,`\n",
    "    values = row.split(',')\n",
    "    \n",
    "    #Filter values\n",
    "    return dict((header[i], values[i]) for i in range(len(header)) if header[i] not in excluding_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess features into usable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Statistics for Z-score normalization\n",
    "def compute_stats(rdd: RDD, feature_indices: list):\n",
    "    # Initialize zeroValue with lists for sums and sums of squares\n",
    "    num_features = len(feature_indices)\n",
    "    zero_value = (0, [0.0] * num_features, [0.0] * num_features)  # (count, sums, sums_of_squares)\n",
    "\n",
    "    # Aggregate to compute count, sum, and sum of squares for each feature\n",
    "    stats = rdd.map(lambda row: [row['features'][i] for i in feature_indices]) \\\n",
    "               .aggregate(zero_value,\n",
    "                          lambda acc, val: (acc[0] + 1,\n",
    "                                           [acc[1][i] + val[i] for i in range(len(val))],\n",
    "                                           [acc[2][i] + val[i]**2 for i in range(len(val))]),\n",
    "                          lambda acc1, acc2: (acc1[0] + acc2[0],\n",
    "                                             [acc1[1][i] + acc2[1][i] for i in range(len(acc1[1]))],\n",
    "                                             [acc1[2][i] + acc2[2][i] for i in range(len(acc1[2]))]))\n",
    "\n",
    "    # Extract count, sums, and sums of squares\n",
    "    count = stats[0]\n",
    "    if count == 0:  # Handle empty RDD\n",
    "        return [0.0] * num_features, [1.0] * num_features\n",
    "\n",
    "    # Compute means\n",
    "    means = [s / count for s in stats[1]]\n",
    "\n",
    "    # Compute variances and standard deviations\n",
    "    variances = [(stats[2][i] / count - (means[i])**2) for i in range(len(means))]\n",
    "    stddevs = [math.sqrt(var) if var > 0 else 1.0 for var in variances]\n",
    "\n",
    "    return means, stddevs\n",
    "\n",
    "def preprocess_data(row: dict, label_col=None, means=None, stddevs=None):\n",
    "    \"\"\"- Flatten `pickup_datetime` into separate elements (day, month, year,...).\n",
    "       - Encode `store_and_fwd_flag` to binary values.\n",
    "       - Cast strings to numeric values.\n",
    "       - Add Haversine distance.\"\"\"\n",
    "    dt = datetime.strptime(row['pickup_datetime'], '%Y-%m-%d %H:%M:%S')\n",
    "    row['store_and_fwd_flag'] = 1 if row['store_and_fwd_flag'] == 'Y' else 0\n",
    "    R = 6371\n",
    "    lon1, lat1 = math.radians(float(row['pickup_longitude'])), math.radians(float(row['pickup_latitude']))\n",
    "    lon2, lat2 = math.radians(float(row['dropoff_longitude'])), math.radians(float(row['dropoff_latitude']))\n",
    "    dlon, dlat = lon2 - lon1, lat2 - lat1\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    distance_km = R * c\n",
    "    features = [\n",
    "        float(row['vendor_id']),\n",
    "        float(dt.year), float(dt.month), float(dt.day), float(dt.hour),\n",
    "        float(dt.minute), float(dt.second),\n",
    "        float(row['passenger_count']),\n",
    "        float(row['pickup_longitude']),\n",
    "        float(row['pickup_latitude']),\n",
    "        float(row['dropoff_longitude']),\n",
    "        float(row['dropoff_latitude']),\n",
    "        row['store_and_fwd_flag'],\n",
    "        distance_km\n",
    "    ]\n",
    "    if means and stddevs:\n",
    "        features = [(features[i] - means[i]) / stddevs[i] for i in range(len(features))]\n",
    "    label = float(row[label_col]) if label_col else None\n",
    "    if label is not None:\n",
    "        label = math.log(label + 1)\n",
    "    return {'id': row.get('id'), 'features': features, 'label': label, 'distance_km': distance_km}\n",
    "\n",
    "# train_rdd = raw_train_rdd.map(lambda row: extract_column(row, train_header, ['id', 'dropoff_datetime']))\\\n",
    "#                          .map(lambda row: preprocess_data(row, 'trip_duration'))\n",
    "\n",
    "    \n",
    "# test_rdd = raw_test_rdd.map(lambda row: extract_column(row, test_header, ['id']))\\\n",
    "#                        .map(lambda row: preprocess_data(row, None)) \n",
    "                \n",
    "# Preprocess data with outlier filtering and Z-score standardization\n",
    "# Step 1: Extract columns and filter outliers for train_rdd\n",
    "train_rdd_raw = raw_train_rdd.map(lambda row: extract_column(row, train_header, ['dropoff_datetime'])).cache()\n",
    "\n",
    "# Filter outliers based on trip_duration and passenger_count\n",
    "train_rdd_filtered = train_rdd_raw.map(lambda row: (row, float(row['trip_duration']), float(row['passenger_count']))) \\\n",
    "                                  .filter(lambda x: (x[1] > 0) and (x[1] < 36000) and (x[2] >= 1) and (x[2] <= 6)) \\\n",
    "                                  .map(lambda x: x[0])\n",
    "\n",
    "# Step 2: Compute distance_km and filter based on distance_km, but keep the original row\n",
    "train_rdd_with_distance = train_rdd_filtered.map(lambda row: (row, preprocess_data(row, 'trip_duration')))\n",
    "train_rdd_filtered = train_rdd_with_distance.filter(lambda x: (x[1]['distance_km'] > 0) and (x[1]['distance_km'] < 100)) \\\n",
    "                                            .map(lambda x: x[1]).cache()\n",
    "\n",
    "# Step 3: Compute stats on filtered data\n",
    "feature_indices = list(range(14))\n",
    "means, stddevs = compute_stats(train_rdd_filtered, feature_indices)\n",
    "\n",
    "# Step 4: Apply preprocessing with Z-score standardization on the original data\n",
    "train_rdd = train_rdd_with_distance.filter(lambda x: (x[1]['distance_km'] > 0) and (x[1]['distance_km'] < 100)) \\\n",
    "                                   .map(lambda x: preprocess_data(x[0], 'trip_duration', means, stddevs))\n",
    "\n",
    "# For test_rdd, apply preprocessing and filtering in one pass\n",
    "test_rdd_raw = raw_test_rdd.map(lambda row: extract_column(row, test_header, [])).cache()\n",
    "test_rdd_with_distance = test_rdd_raw.map(lambda row: (row, preprocess_data(row, None)))\n",
    "test_rdd = test_rdd_with_distance.filter(lambda x: (x[1]['distance_km'] > 0) and (x[1]['distance_km'] < 100)) \\\n",
    "                                 .map(lambda x: preprocess_data(x[0], None, means, stddevs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split train/validation \n",
    "train_rdd, val_rdd = train_rdd.randomSplit([0.8, 0.2], seed=24)\n",
    "train_rdd = train_rdd.cache()\n",
    "val_rdd = val_rdd.cache()\n",
    "test_rdd = test_rdd.cache()\n",
    "\n",
    "# Free up memory\n",
    "train_rdd_raw.unpersist()\n",
    "train_rdd_filtered.unpersist()\n",
    "test_rdd_raw.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`partition_feature_grouping` for flattening each row of dataset into a tuple of (feature_value, row_label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_feature_grouping(partition, usable_features: list):\n",
    "    \"\"\"Read each row and convert into a list of (feature_value, row_label), then flatten the results.\"\"\"\n",
    "    feature_stats = {}\n",
    "    for row in partition:\n",
    "        for feature in usable_features:\n",
    "            # Store feature-wise values in a dictionary\n",
    "            if feature not in feature_stats:\n",
    "                feature_stats[feature] = []\n",
    "\n",
    "            feature_stats[feature].append((row['features'][feature], row['label']))\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for feature, values in feature_stats.items():\n",
    "        results.append((feature, values))\n",
    "\n",
    "    return iter(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_split_feature` for finding splitting point with maximum variance reduction on the domain of a feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split_feature(values: list, parent_info: StatCounter):\n",
    "    # Sort values by feature value (ascending order)\n",
    "    sorted_values = sorted(values, key=lambda x: x[0])\n",
    "\n",
    "    # Get total number of data points\n",
    "    parent_count = parent_info.count()\n",
    "\n",
    "    # Get variance, mean, and sum of the whole dataset\n",
    "    parent_var = parent_info.variance()\n",
    "    parent_mean = parent_info.mean()\n",
    "    parent_sum = parent_info.sum()\n",
    "\n",
    "    # Calculate parent's sum of squares: Σx_i^2 = n * (Var(X) + E[X]^2)\n",
    "    parent_pow_sum = parent_count * (parent_var + parent_mean ** 2)\n",
    "\n",
    "    # If not enough data to split, return no split\n",
    "    if parent_count < 2:\n",
    "        return (-float(\"inf\"), None)\n",
    "\n",
    "    # Initialize variables for left partition\n",
    "    left_sum, left_pow_sum, left_count = 0, 0, 0\n",
    "    best_split = (-float(\"inf\"), None)\n",
    "\n",
    "    # Iterate through possible split points\n",
    "    for i in range(parent_count - 1):\n",
    "        val, label = sorted_values[i]\n",
    "\n",
    "        # Accumulate stats for left partition\n",
    "        left_sum += label\n",
    "        left_pow_sum += label ** 2\n",
    "        left_count += 1\n",
    "\n",
    "        # Skip splitting between identical feature values\n",
    "        if val == sorted_values[i + 1][0]:\n",
    "            continue\n",
    "\n",
    "        # Compute right partition stats by subtracting left from parent\n",
    "        right_count = parent_count - left_count\n",
    "        right_sum = parent_sum - left_sum\n",
    "        right_pow_sum = parent_pow_sum - left_pow_sum\n",
    "\n",
    "        # Compute variance for each partition\n",
    "        left_var = (left_pow_sum / left_count - (left_sum / left_count) ** 2) if left_count > 0 else 0\n",
    "        right_var = (right_pow_sum / right_count - (right_sum / right_count) ** 2) if right_count > 0 else 0\n",
    "\n",
    "        # Calculate weighted variance of children\n",
    "        weighted_child_var = (left_var * left_count + right_var * right_count) / parent_count\n",
    "\n",
    "        # Compute variance reduction\n",
    "        var_reduction = parent_var - weighted_child_var\n",
    "\n",
    "        # Update best split if variance reduction is positive and greater than current best\n",
    "        if var_reduction > 0 and var_reduction > best_split[0]:\n",
    "            best_split = (var_reduction, (val + sorted_values[i + 1][0]) / 2)\n",
    "\n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-function `splitter` for splitting the rows of the parent dataset according to the splitting point and operands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(iterator, split_feature: int, split_point: float, operand):\n",
    "    ret = []\n",
    "    for row in iterator:\n",
    "        if operand(row['features'][split_feature], split_point):\n",
    "            ret.append(row)\n",
    "\n",
    "    return iter(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main class `DecisionTreeRegressor` for building and executing Decision Tree Regressor Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synchronized with maxDepth=10 from sections 3.2.1 and 3.2.2 (other parameters like maxBins and minInstancesPerNode cannot be directly applied in manual implementation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth = 10): # Synchronized with maxDepth=10 from sections 3.2.1 and 3.2.2\n",
    "        #Initialize the estimator with given depth (if any)\n",
    "        self.max_depth = max_depth\n",
    "        self.rules = None\n",
    "        self.num_features = None\n",
    "\n",
    "    def set_maxDepth(self, depth):\n",
    "        \"\"\"Set current maximum depth for the estimator.\"\"\"\n",
    "        self.max_depth = depth\n",
    "\n",
    "    def fit(self, train_rdd: RDD):\n",
    "        \"\"\"Execute Decision Tree Algorithm recursively on a given rdd based on variance reduction and the current maximum depth.\"\"\"\n",
    "        self.num_features = len(train_rdd.first()['features'])\n",
    "        self.usable_features = [i for i in range(self.num_features)]\n",
    "        sample_size = train_rdd.count()\n",
    "        print(f\"Starting tree construction with {sample_size} samples, max_depth={self.max_depth}\")\n",
    "        self.rules = self.__build_rule_tree_recursive(train_rdd, self.usable_features, sample_size)\n",
    "        print(\"Tree construction completed\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, rdd: RDD):\n",
    "        \"\"\"Make predictions on an RDD using the decision tree rules.\"\"\"\n",
    "        def predict_row(row):\n",
    "            # Start from the root of the decision tree\n",
    "            node = self.rules\n",
    "\n",
    "            # Traverse the tree until reaching a leaf node (which contains a prediction)\n",
    "            while 'prediction' not in node:\n",
    "                # Compare the feature value with the split point to decide the direction\n",
    "                if row['features'][node['split_feature']] <= node['split_point']:\n",
    "                    node = node['left']  # Go to the left subtree\n",
    "                else:\n",
    "                    node = node['right']  # Go to the right subtree\n",
    "            prediction = math.exp(node['prediction']) - 1  # Chuyển ngược từ log\n",
    "            # Return a tuple of (row ID, predicted value)\n",
    "            return (row['id'], prediction)\n",
    "\n",
    "        # Apply prediction to each row in the RDD\n",
    "        return rdd.map(lambda row: predict_row(row))\n",
    "    \n",
    "    def evaluate(self, rdd: RDD):\n",
    "        # Evaluate the model using RMSE and R² on a dataset with labels\n",
    "        predictions = self.transform(rdd)\n",
    "        actual_and_pred = predictions.join(rdd.map(lambda row: (row['id'], math.exp(row['label']) - 1)))\n",
    "\n",
    "        # Compute RMSE\n",
    "        mse = actual_and_pred.map(lambda x: (x[1][0] - x[1][1]) ** 2).mean()\n",
    "        rmse = math.sqrt(mse)\n",
    "\n",
    "        # Compute R²\n",
    "        # Step 1: Compute mean of actual labels (y_bar)\n",
    "        actual_labels = rdd.map(lambda row: math.exp(row['label']) - 1)\n",
    "        y_mean = actual_labels.mean()\n",
    "        \n",
    "        # Step 2: Compute SS_tot (total sum of squares)\n",
    "        ss_tot = actual_labels.map(lambda y: (y - y_mean) ** 2).sum()\n",
    "        \n",
    "        # Step 3: Compute SS_res (sum of squared residuals)\n",
    "        ss_res = actual_and_pred.map(lambda x: (x[1][0] - x[1][1]) ** 2).sum()\n",
    "        \n",
    "        # Step 4: Compute R²\n",
    "        r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n",
    "\n",
    "        return rmse, r2\n",
    "\n",
    "    def display_rule_tree(self):\n",
    "        \"\"\"Recursively display the rules of a Decision Tree model.\"\"\"\n",
    "        print(\"\\nDecision Tree Rules:\")\n",
    "        self.__display_tree_recursive(self.rules)\n",
    "\n",
    "    def __display_tree_recursive(self, rules: dict, indent = 0):\n",
    "        #Stopping condition\n",
    "        if not rules:\n",
    "            return\n",
    "\n",
    "        if 'prediction' in rules:  #Is a leaf condition\n",
    "            prediction = math.exp(rules['prediction']) - 1  # Reverse from log\n",
    "            print(f\"{indent * ' '}Predict: {prediction:.2f}\")\n",
    "            return\n",
    "\n",
    "        #Print the splitting point information and call recursion of the left and right child\n",
    "        print(f\"{indent * ' '}If feature[{rules['split_feature']}] <= {rules['split_point']:.2f}\")\n",
    "        self.__display_tree_recursive(rules['left'], indent + 4)\n",
    "        print(f\"{indent * ' '}Else feature[{rules['split_feature']}] > {rules['split_point']:.2f}\")\n",
    "        self.__display_tree_recursive(rules['right'], indent + 4)      \n",
    "\n",
    "    def __find_best_split(self, rdd: RDD, usable_features: list):         \n",
    "        \"\"\"Find the best splitting point with maximum variance reduction of the input dataset.\"\"\"\n",
    "\n",
    "        #Compute the dataset statistics and store into a StatCounter object\n",
    "        parent_info = rdd.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2))\n",
    "        \n",
    "        #Convert each row into a list of (row_feature_value, row_label) and flatten the results\n",
    "        processed_rdd = rdd.mapPartitions(lambda partition: partition_feature_grouping(partition, usable_features), True)\\\n",
    "                      .reduceByKey(lambda l1, l2: l1 + l2).cache()\n",
    "        \n",
    "        #Find the best splitting point for each features\n",
    "        best_split_per_feature = processed_rdd.mapValues(lambda values: find_split_feature(values, parent_info))\\\n",
    "                                  \n",
    "        #Find the best splitting point for the dataset\n",
    "        best_split = max(best_split_per_feature.collect(),key=lambda x: (x[1][0], -x[0]))\n",
    "\n",
    "        return best_split[0], best_split[1][1]\n",
    "\n",
    "\n",
    "    def __build_rule_tree_recursive(self, parent: RDD, usable_features: list, sample_size, depth = 0):\n",
    "        \"\"\"Recursively build the decision tree by finding the splitting point with maximum variance reduction and split the dataset with this point, up to the maximum depth.\"\"\"\n",
    "        print(f\"Building tree at depth {depth}, sample size: {sample_size}\")\n",
    "        #Stopping condition\n",
    "        if sample_size == 0:\n",
    "            return None\n",
    "        \n",
    "        #Return the mean of un-splitted subset as the prediction if reached the depth limit\n",
    "        if depth == self.max_depth or sample_size < 2:\n",
    "            mean = parent.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2)).mean()\n",
    "\n",
    "            return {'prediction' : mean}\n",
    "\n",
    "        #Find the best splitting point for the input dataset\n",
    "        split_feature, split_point = self.__find_best_split(parent, usable_features)\n",
    "        \n",
    "        #Return the mean of un-splitted subset as the prediction if no splitting point is valid but has not reached the depth limit yet\n",
    "        if split_point == None:\n",
    "            mean = parent.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2)).mean()\n",
    "            \n",
    "            return {'prediction' : mean}\n",
    "\n",
    "        #Split the dataset into left and right subsets\n",
    "        left_rdd = parent.mapPartitions(lambda iterator: splitter(iterator, split_feature, split_point, lambda a,b: a <= b), True).cache()\n",
    "        right_rdd = parent.mapPartitions(lambda iterator: splitter(iterator, split_feature, split_point, lambda a,b: a >= b), True).cache()\n",
    "        \n",
    "        #Un-cache the parent dataset (excluding the input one)\n",
    "        if depth > 0:\n",
    "            parent.unpersist()\n",
    "\n",
    "        #Compute the size of the left subset\n",
    "        left_sample_size = left_rdd.count()\n",
    "     \n",
    "        #Build the dict of information with recursion call\n",
    "        return {\n",
    "            'split_feature' : split_feature,\n",
    "            'split_point' : split_point,\n",
    "            'left' : self.__build_rule_tree_recursive(left_rdd, usable_features, left_sample_size, depth + 1),\n",
    "            'right' : self.__build_rule_tree_recursive(right_rdd, usable_features, sample_size - left_sample_size, depth + 1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/12 09:27:12 WARN BlockManager: Task 8 already completed, not releasing lock for rdd_10_0\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tree construction with 1161023 samples, max_depth=5\n",
      "Building tree at depth 0, sample size: 1161023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 1, sample size: 579030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 2, sample size: 188790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 3, sample size: 3008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 1671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 1050\n",
      "Building tree at depth 5, sample size: 621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 1337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 1216\n",
      "Building tree at depth 5, sample size: 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 3, sample size: 185782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 54252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 53836\n",
      "Building tree at depth 5, sample size: 416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 131530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 98094\n",
      "Building tree at depth 5, sample size: 33436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 2, sample size: 390240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 3, sample size: 185344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 147755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 22860\n",
      "Building tree at depth 5, sample size: 124895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 37589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 19289\n",
      "Building tree at depth 5, sample size: 18300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 3, sample size: 204896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 31009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 23080\n",
      "Building tree at depth 5, sample size: 7929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 173887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 135719\n",
      "Building tree at depth 5, sample size: 38168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 1, sample size: 581993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 2, sample size: 385334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 3, sample size: 213226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 36217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 19082\n",
      "Building tree at depth 5, sample size: 17135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 177009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 138309\n",
      "Building tree at depth 5, sample size: 38700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 3, sample size: 172108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 34957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 19428\n",
      "Building tree at depth 5, sample size: 15529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 137151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 92529\n",
      "Building tree at depth 5, sample size: 44622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 2, sample size: 196659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 3, sample size: 148444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 91295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 24376\n",
      "Building tree at depth 5, sample size: 66919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 57149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 12100\n",
      "Building tree at depth 5, sample size: 45049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 3, sample size: 48215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 21121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 3963\n",
      "Building tree at depth 5, sample size: 17158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 27094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 2958\n",
      "Building tree at depth 5, sample size: 24136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 160:======================================>                  (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree construction completed\n",
      "\n",
      "Decision Tree Rules:\n",
      "If feature[13] <= -0.34\n",
      "    If feature[13] <= -0.62\n",
      "        If feature[13] <= -0.87\n",
      "            If feature[13] <= -0.87\n",
      "                If feature[13] <= -0.87\n",
      "                    Predict: 17.67\n",
      "                Else feature[13] > -0.87\n",
      "                    Predict: 24.29\n",
      "            Else feature[13] > -0.87\n",
      "                If feature[8] <= 2.32\n",
      "                    Predict: 65.94\n",
      "                Else feature[8] > 2.32\n",
      "                    Predict: 19.05\n",
      "        Else feature[13] > -0.87\n",
      "            If feature[13] <= -0.72\n",
      "                If feature[8] <= 2.34\n",
      "                    Predict: 203.02\n",
      "                Else feature[8] > 2.34\n",
      "                    Predict: 49.66\n",
      "            Else feature[13] > -0.72\n",
      "                If feature[11] <= 0.49\n",
      "                    Predict: 318.73\n",
      "                Else feature[11] > 0.49\n",
      "                    Predict: 240.49\n",
      "    Else feature[13] > -0.62\n",
      "        If feature[13] <= -0.50\n",
      "            If feature[11] <= 0.55\n",
      "                If feature[4] <= -0.95\n",
      "                    Predict: 329.87\n",
      "                Else feature[4] > -0.95\n",
      "                    Predict: 465.02\n",
      "            Else feature[11] > 0.55\n",
      "                If feature[13] <= -0.56\n",
      "                    Predict: 302.35\n",
      "                Else feature[13] > -0.56\n",
      "                    Predict: 355.23\n",
      "        Else feature[13] > -0.50\n",
      "            If feature[4] <= -0.95\n",
      "                If feature[4] <= -1.11\n",
      "                    Predict: 412.88\n",
      "                Else feature[4] > -1.11\n",
      "                    Predict: 484.35\n",
      "            Else feature[4] > -0.95\n",
      "                If feature[11] <= 0.60\n",
      "                    Predict: 622.80\n",
      "                Else feature[11] > 0.60\n",
      "                    Predict: 465.45\n",
      "Else feature[13] > -0.34\n",
      "    If feature[13] <= 0.46\n",
      "        If feature[13] <= -0.07\n",
      "            If feature[4] <= -0.95\n",
      "                If feature[13] <= -0.21\n",
      "                    Predict: 534.29\n",
      "                Else feature[13] > -0.21\n",
      "                    Predict: 616.97\n",
      "            Else feature[4] > -0.95\n",
      "                If feature[11] <= 0.68\n",
      "                    Predict: 812.19\n",
      "                Else feature[11] > 0.68\n",
      "                    Predict: 648.12\n",
      "        Else feature[13] > -0.07\n",
      "            If feature[4] <= -0.95\n",
      "                If feature[13] <= 0.17\n",
      "                    Predict: 718.38\n",
      "                Else feature[13] > 0.17\n",
      "                    Predict: 853.53\n",
      "            Else feature[4] > -0.95\n",
      "                If feature[4] <= 0.92\n",
      "                    Predict: 1118.72\n",
      "                Else feature[4] > 0.92\n",
      "                    Predict: 916.76\n",
      "    Else feature[13] > 0.46\n",
      "        If feature[13] <= 2.29\n",
      "            If feature[13] <= 1.21\n",
      "                If feature[4] <= -0.95\n",
      "                    Predict: 1076.50\n",
      "                Else feature[4] > -0.95\n",
      "                    Predict: 1363.01\n",
      "            Else feature[13] > 1.21\n",
      "                If feature[4] <= -1.11\n",
      "                    Predict: 1337.54\n",
      "                Else feature[4] > -1.11\n",
      "                    Predict: 1751.97\n",
      "        Else feature[13] > 2.29\n",
      "            If feature[13] <= 3.71\n",
      "                If feature[4] <= -1.27\n",
      "                    Predict: 1609.67\n",
      "                Else feature[4] > -1.27\n",
      "                    Predict: 2092.18\n",
      "            Else feature[13] > 3.71\n",
      "                If feature[4] <= -1.27\n",
      "                    Predict: 1831.13\n",
      "                Else feature[4] > -1.27\n",
      "                    Predict: 2786.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Can not train with max_depth=10\n",
    "estimator = DecisionTreeRegressor(max_depth=5) \n",
    "model = estimator.fit(train_rdd)\n",
    "estimator.display_rule_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 431.81822822875625\n",
      "Validation R²: 0.6010753112901581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "val_rmse, val_r2 = model.evaluate(val_rdd)\n",
    "print(f\"Validation RMSE: {val_rmse}\")\n",
    "print(f\"Validation R²: {val_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample predictions for a few test cases (test file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/12 09:35:04 WARN BlockManager: Task 999 already completed, not releasing lock for rdd_12_0\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: id3004672, Prediction: 812.191225632504\n",
      "ID: id3505355, Prediction: 812.191225632504\n",
      "ID: id1217141, Prediction: 465.0235860725819\n",
      "ID: id2150126, Prediction: 1363.012524057769\n",
      "ID: id1598245, Prediction: 318.7302742288561\n"
     ]
    }
   ],
   "source": [
    "# Predict and display samples\n",
    "predictions_rdd = estimator.transform(test_rdd)\n",
    "print(\"Sample Predictions:\")\n",
    "predictions_samples = predictions_rdd.take(5)\n",
    "for pred in predictions_samples:\n",
    "    print(f\"ID: {pred[0]}, Prediction: {pred[1]}\")\n",
    "\n",
    "# Stop SparkContext\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
