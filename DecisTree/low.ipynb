{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, RDD\n",
    "from pyspark.statcounter import StatCounter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SparkConf()\\\n",
    "            .set(\"spark.driver.memory\", \"4g\")\\\n",
    "            .set(\"spark.executor.memory\", \"8g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/04/03 00:34:28 WARN Utils: Your hostname, DESKTOP-0H87CFM resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/03 00:34:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/03 00:34:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(appName='taxi_duration_lowlevel', conf=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Convert data into rdd\n",
    "raw_rdd = sc.textFile('train.csv')\n",
    "\n",
    "#Remove the csv header row\n",
    "header = raw_rdd.first()\n",
    "\n",
    "#Strip the header row\n",
    "rdd_no_header = raw_rdd.filter(lambda row: row != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(row: str):\n",
    "    \"\"\"Vectorize the features and labels.\"\"\"\n",
    "\n",
    "    #Split the string for elements, excluding the first feature (`id`)\n",
    "    values = [item for item in row.split(',')]\n",
    "\n",
    "    #Split the timestamps features into time elements\n",
    "    pickup_time = datetime.strptime(values[2], '%Y-%m-%d %H:%M:%S').strftime('%Y:%m:%d:%H:%M:%S').split(':')\n",
    "    dropoff_time = datetime.strptime(values[3], '%Y-%m-%d %H:%M:%S').strftime('%Y:%m:%d:%H:%M:%S').split(':')\n",
    "    \n",
    "    #Encode the `store_and_fwd_flag` into binary values\n",
    "    values[9] = 1 if values[9] == 'Y' else 0\n",
    "\n",
    "    #Cast strings into number\n",
    "    raw_vector = [float(value) for value in [values[1]] + pickup_time + dropoff_time + values[4:]]\n",
    "    \n",
    "    #Build and return the LabeledPoint-like row\n",
    "    return {'label': raw_vector[-1], 'features' : tuple(raw_vector[:-1])}\n",
    "\n",
    "rdd = rdd_no_header.map(vectorize).repartition(8).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_feature_grouping(partition, usable_features: list):\n",
    "    feature_stats = {}\n",
    "\n",
    "    for row in partition:\n",
    "        for feature in usable_features:\n",
    "            # Store feature-wise values in a dictionary\n",
    "            if feature not in feature_stats:\n",
    "                feature_stats[feature] = []\n",
    "\n",
    "            feature_stats[feature].append((row['features'][feature], row['label']))\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for feature, values in feature_stats.items():\n",
    "        results.append((feature, values))\n",
    "\n",
    "    return iter(results)\n",
    "\n",
    "\n",
    "def find_feature_best_split(values: list[list[float ] ], parent_info: StatCounter):\n",
    "            sorted_values = sorted(values)\n",
    "\n",
    "            parent_count = parent_info.count()\n",
    "            parent_pow_sum = parent_info.variance() + parent_info.mean() ** 2\n",
    "            \n",
    "            best_split = (-float(\"inf\"), None) \n",
    "\n",
    "\n",
    "            left_sum, left_pow_sum, left_count = 0, 0, 0\n",
    "            \n",
    "            for i in range(0, parent_count - 1):\n",
    "                feature_value, label = sorted_values[i]\n",
    "\n",
    "                left_sum += label\n",
    "                left_pow_sum += label ** 2\n",
    "                left_count += 1\n",
    "\n",
    "                if feature_value == sorted_values[i+1][0]:\n",
    "                    continue\n",
    "\n",
    "                right_count = parent_count - left_count\n",
    "\n",
    "                right_sum = parent_info.sum() - left_sum\n",
    "                right_pow_sum = parent_pow_sum - left_pow_sum\n",
    "\n",
    "                \n",
    "                left_var = left_pow_sum / left_count - (left_sum / left_count)**2 if left_count > 0 else 0\n",
    "                right_var = right_pow_sum / right_count - (right_sum / right_count)**2 if right_count > 0 else 0\n",
    "\n",
    "                var_reduction = parent_info.variance() - left_var * (left_count / parent_count) - right_var * (right_count / parent_count)     \n",
    "\n",
    "                \n",
    "                if var_reduction > best_split[0]:\n",
    "                    best_split = ( var_reduction, (feature_value, sorted_values[i+1][0]) )\n",
    "\n",
    "            return best_split\n",
    "\n",
    "\n",
    "def splitter(iterator, split_feature: int, split_point: float, operand):\n",
    "    ret = []\n",
    "    for row in iterator:\n",
    "        if operand(row['features'][split_feature], split_point):\n",
    "            ret.append(row)\n",
    "\n",
    "    return iter(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth = 1):\n",
    "        self.max_depth = max_depth\n",
    "        self.rules = None\n",
    "        pass\n",
    "\n",
    "    def set_maxDepth(self, depth):\n",
    "        self.max_depth = depth\n",
    "\n",
    "    def fit(self, train_rdd: RDD):\n",
    "        self.num_features = len(train_rdd.first()['features'])\n",
    "        self.usable_features = [i for i in range(self.num_features)]\n",
    "        sample_size = train_rdd.count()\n",
    "        \n",
    "        return self.__build_rule_tree_recursive(train_rdd, self.usable_features, sample_size)\n",
    "\n",
    "    def transform(self, rdd: RDD):\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def display_rule_tree(self, model: dict):\n",
    "        self.__display_tree_recusive(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __display_tree_recusive(self, rules: dict, indent = 0):\n",
    "        if not rules:\n",
    "            return\n",
    "\n",
    "        print(f\"{indent * ' '}If  features[{rules['split_feature']}] <= {rules['split_point']}\")\n",
    "\n",
    "        if len(rules['left']) == 1:\n",
    "            print(f\"{indent  * ' ' + '   '}Predict: {rules['left']['prediction']}\")\n",
    "        else:\n",
    "            self.__display_tree_recusive(rules['left'], indent + 5)\n",
    "        \n",
    "        if rules['right']:\n",
    "            print(f\"{indent * ' '}Else  features[{rules['split_feature']}] > {rules['split_point']}\")\n",
    "\n",
    "            if len(rules['right']) == 1:\n",
    "                print(f\"{indent * ' ' + '   '}Predict: {rules['right']['prediction']}\")\n",
    "\n",
    "            else:\n",
    "                self.__display_tree_recusive(rules['right'], indent + 5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __find_best_split(self, rdd: RDD, usable_features: list):         \n",
    "        \n",
    "        parent_info = rdd.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2))\n",
    "        \n",
    "\n",
    "        processed_rdd = rdd.mapPartitions(lambda partition: partition_feature_grouping(partition, usable_features), True)\\\n",
    "                      .reduceByKey(lambda l1, l2: l1 + l2).cache()\n",
    "        \n",
    "\n",
    "        best_split_per_feature = processed_rdd.mapValues(lambda values: find_feature_best_split(values, parent_info))\\\n",
    "                                  \n",
    "\n",
    "        best_split = max(best_split_per_feature.collect(),key=lambda x: (x[1][0], -x[0]))\n",
    "\n",
    "\n",
    "        return best_split[0], best_split[1][1]\n",
    "\n",
    "\n",
    "    def __build_rule_tree_recursive(self, parent: RDD, usable_features: list, sample_size, depth = 0):\n",
    "        if sample_size == 0:\n",
    "            return None\n",
    "        \n",
    "        if depth == self.max_depth:\n",
    "            mean = parent.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2)).mean()\n",
    "\n",
    "            return {'prediction' : mean}\n",
    "\n",
    "        split_feature, split_bound = self.__find_best_split(parent, usable_features)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        if split_feature == None or split_bound == None:\n",
    "            mean = parent.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2)).mean()\n",
    "            \n",
    "            return {'prediction' : mean}\n",
    "\n",
    "        mid_point = (split_bound[0] + split_bound[1]) / 2\n",
    "\n",
    "        left_rdd = parent.mapPartitions(lambda iterator: splitter(iterator, split_feature, mid_point, lambda a,b: a <= b), True).cache()\n",
    "        right_rdd = parent.mapPartitions(lambda iterator: splitter(iterator, split_feature, mid_point, lambda a,b: a >= b), True).cache()\n",
    "        \n",
    "        if depth > 0:\n",
    "            parent.unpersist()\n",
    "\n",
    "\n",
    "\n",
    "        left_sample_size = left_rdd.count()\n",
    "     \n",
    "        return {\n",
    "            'split_feature' : split_feature,\n",
    "            'split_point' : mid_point,\n",
    "            'left' : self.__build_rule_tree_recursive(left_rdd, usable_features, left_sample_size, depth + 1),\n",
    "            'right' : self.__build_rule_tree_recursive(right_rdd, usable_features, sample_size - left_sample_size, depth + 1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/03 00:37:36 WARN BlockManager: Task 584 already completed, not releasing lock for rdd_7_0\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If  features[14] <= -73.88602066040039\n",
      "     If  features[16] <= -73.91103744506836\n",
      "          If  features[17] <= 40.70149040222168\n",
      "               If  features[15] <= 40.725751876831055\n",
      "                    If  features[17] <= 40.701486587524414\n",
      "                       Predict: 1019.1713947990552\n",
      "                    Else  features[17] > 40.701486587524414\n",
      "                       Predict: 28934.666666666668\n",
      "               Else  features[15] > 40.725751876831055\n",
      "                    If  features[15] <= 40.72575569152832\n",
      "                       Predict: 22868.0\n",
      "                    Else  features[15] > 40.72575569152832\n",
      "                       Predict: 1901.497137558458\n",
      "          Else  features[17] > 40.70149040222168\n",
      "               If  features[0] <= 1.5\n",
      "                    If  features[14] <= -73.92168045043945\n",
      "                       Predict: 712.2167951813683\n",
      "                    Else  features[14] > -73.92168045043945\n",
      "                       Predict: 2017.9384793964011\n",
      "               Else  features[0] > 1.5\n",
      "                    If  features[8] <= 6.5\n",
      "                       Predict: 919.3201688158626\n",
      "                    Else  features[8] > 6.5\n",
      "                       Predict: 19500.17391304348\n",
      "     Else  features[16] > -73.91103744506836\n",
      "          If  features[16] <= -73.81863784790039\n",
      "               If  features[14] <= -73.95466995239258\n",
      "                    If  features[8] <= 6.5\n",
      "                       Predict: 1925.2810879933654\n",
      "                    Else  features[8] > 6.5\n",
      "                       Predict: 30490.0\n",
      "               Else  features[14] > -73.95466995239258\n",
      "                    If  features[14] <= -73.92405319213867\n",
      "                       Predict: 1275.0604395604391\n",
      "                    Else  features[14] > -73.92405319213867\n",
      "                       Predict: 672.9857090509345\n",
      "          Else  features[16] > -73.81863784790039\n",
      "               If  features[16] <= -73.818603515625\n",
      "                  Predict: 86171.0\n",
      "               Else  features[16] > -73.818603515625\n",
      "                    If  features[4] <= 12.5\n",
      "                       Predict: 2364.1227122189302\n",
      "                    Else  features[4] > 12.5\n",
      "                       Predict: 3418.5374537168896\n",
      "Else  features[14] > -73.88602066040039\n",
      "     If  features[16] <= -73.95637893676758\n",
      "          If  features[15] <= 40.682716369628906\n",
      "               If  features[16] <= -73.95681762695312\n",
      "                    If  features[15] <= 40.648630142211914\n",
      "                       Predict: 3080.9053322395416\n",
      "                    Else  features[15] > 40.648630142211914\n",
      "                       Predict: 5649.6625598904875\n",
      "               Else  features[16] > -73.95681762695312\n",
      "                    If  features[16] <= -73.95680618286133\n",
      "                       Predict: 647897.6666666666\n",
      "                    Else  features[16] > -73.95680618286133\n",
      "                       Predict: 3050.98\n",
      "          Else  features[15] > 40.682716369628906\n",
      "               If  features[4] <= 18.5\n",
      "                    If  features[4] <= 6.5\n",
      "                       Predict: 1428.2099343955017\n",
      "                    Else  features[4] > 6.5\n",
      "                       Predict: 2443.4453028081634\n",
      "               Else  features[4] > 18.5\n",
      "                    If  features[16] <= -73.98086166381836\n",
      "                       Predict: 1834.5476923076926\n",
      "                    Else  features[16] > -73.98086166381836\n",
      "                       Predict: 1468.7130336443308\n",
      "     Else  features[16] > -73.95637893676758\n",
      "          If  features[16] <= -73.90579223632812\n",
      "               If  features[14] <= -73.81616592407227\n",
      "                    If  features[17] <= 40.69793128967285\n",
      "                       Predict: 2139.394014962594\n",
      "                    Else  features[17] > 40.69793128967285\n",
      "                       Predict: 1299.974324324324\n",
      "               Else  features[14] > -73.81616592407227\n",
      "                    If  features[4] <= 18.5\n",
      "                       Predict: 2558.322143864598\n",
      "                    Else  features[4] > 18.5\n",
      "                       Predict: 2021.915341200616\n",
      "          Else  features[16] > -73.90579223632812\n",
      "               If  features[13] <= 0.5\n",
      "                    If  features[0] <= 1.5\n",
      "                       Predict: 5.75\n",
      "                    Else  features[0] > 1.5\n",
      "                       Predict: 85901.0\n",
      "               Else  features[13] > 0.5\n",
      "                    If  features[17] <= 40.779428482055664\n",
      "                       Predict: 1097.0902861058762\n",
      "                    Else  features[17] > 40.779428482055664\n",
      "                       Predict: 1876.4273858921165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "estimator = DecisionTreeRegressor(5)\n",
    "model = estimator.fit(rdd)\n",
    "estimator.display_rule_tree(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrjob_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
