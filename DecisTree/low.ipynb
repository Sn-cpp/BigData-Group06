{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor with Pyspark low-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, RDD\n",
    "from pyspark.statcounter import StatCounter\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Set the memory usage configurations for Pyspark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the config for spark to enhance performance\n",
    "config = SparkConf()\\\n",
    "            .set(\"spark.driver.memory\", \"4g\")\\\n",
    "            .set(\"spark.executor.memory\", \"4g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize a Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/04/11 23:34:43 WARN Utils: Your hostname, DESKTOP-0H87CFM resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/11 23:34:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/11 23:34:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#pyspark init\n",
    "sc = SparkContext(appName='taxi_duration_lowlevel', conf=config).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def read_csv(filepath: str):\n",
    "    \"\"\"Read csv file into a rdd of values and a list of feature columns separately.\"\"\"\n",
    "    #Read the data into rdd\n",
    "    raw_rdd = sc.textFile(filepath)\n",
    "\n",
    "    #Remove the csv header row\n",
    "    header = raw_rdd.first()\n",
    "\n",
    "    #Strip the header row\n",
    "    rdd_no_header = raw_rdd.filter(lambda row: row != header)\n",
    "\n",
    "    return rdd_no_header, header.split(',')\n",
    "\n",
    "\n",
    "raw_train_rdd, train_header = read_csv('train.csv')\n",
    "raw_test_rdd, test_header = read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter usable columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_column(row: str, header: list[str], excluding_features: list[str]):\n",
    "    \"\"\"Extract usable feature columns with given excluding filter.\"\"\"\n",
    "    #Split the row with delimiter `,`\n",
    "    values = row.split(',')\n",
    "    \n",
    "    #Filter values\n",
    "    return dict((header[i], values[i]) for i in range(len(header)) if header[i] not in excluding_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess features into usable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_data(row: dict, label_col = None):\n",
    "    \"\"\"- Flatten `pickup_datetime` into separate elements (day, month, year,...).\n",
    "       - Encode `store_and_fwd_flag` to binary values.\n",
    "       - Cast strings to numeric values.\n",
    "       - Add Haversine distance.\"\"\"\n",
    "\n",
    "    #dict_row = {}\n",
    "\n",
    "    #Flatten datetime strings into component elements\n",
    "    #row['pickup_datetime'] = row['pickup_datetime'].replace('-', ':').replace(' ', ':').split(':')\n",
    "    dt = datetime.strptime(row['pickup_datetime'], '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    #Encode binary values feature\n",
    "    row['store_and_fwd_flag'] = 1 if row['store_and_fwd_flag'] == 'Y' else 0\n",
    "    \n",
    "    # Add Haversine distance\n",
    "    R = 6371\n",
    "    lon1, lat1 = math.radians(float(row['pickup_longitude'])), math.radians(float(row['pickup_latitude']))\n",
    "    lon2, lat2 = math.radians(float(row['dropoff_longitude'])), math.radians(float(row['dropoff_latitude']))\n",
    "    dlon, dlat = lon2 - lon1, lat2 - lat1\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    distance_km = R * c\n",
    "    \n",
    "    #Cast strings to float\n",
    "    features = [float(value) for value in [\n",
    "                    float(row['vendor_id']),\n",
    "                    float(dt.year), float(dt.month), float(dt.day), float(dt.hour),\n",
    "                    float(dt.minute), float(dt.second),\n",
    "                    float(row['passenger_count']),\n",
    "                    float(row['pickup_longitude']), \n",
    "                    float(row['pickup_latitude']),\n",
    "                    float(row['dropoff_longitude']), \n",
    "                    float(row['dropoff_latitude']),\n",
    "                    row['store_and_fwd_flag'],\n",
    "                    distance_km\n",
    "                ]\n",
    "        ]\n",
    "    \n",
    "    #Return a dict of { 'features': vector_of_values, 'label': label (if any) }\n",
    "    # dict_row['features'] = features\n",
    "    # dict_row['label'] = float(row[label_col]) if label_col else None\n",
    "\n",
    "    return {'id': row.get('id'), 'features': features, 'label': float(row[label_col]) if label_col else None}\n",
    "\n",
    "# train_rdd = raw_train_rdd.map(lambda row: extract_column(row, train_header, ['id', 'dropoff_datetime']))\\\n",
    "#                          .map(lambda row: preprocess_data(row, 'trip_duration'))\n",
    "\n",
    "    \n",
    "# test_rdd = raw_test_rdd.map(lambda row: extract_column(row, test_header, ['id']))\\\n",
    "#                        .map(lambda row: preprocess_data(row, None))                 \n",
    "train_rdd = raw_train_rdd.map(lambda row: extract_column(row, train_header, ['dropoff_datetime'])) \\\n",
    "                         .map(lambda row: preprocess_data(row, 'trip_duration'))\n",
    "                         \n",
    "test_rdd = raw_test_rdd.map(lambda row: extract_column(row, test_header, [])) \\\n",
    "                       .map(lambda row: preprocess_data(row))\n",
    "                       \n",
    "# Cache to reuse\n",
    "train_rdd.cache()\n",
    "test_rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`partition_feature_grouping` for flattening each row of dataset into a tuple of (feature_value, row_label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_feature_grouping(partition, usable_features: list):\n",
    "    \"\"\"Read each row and convert into a list of (feature_value, row_label), then flatten the results.\"\"\"\n",
    "    feature_stats = {}\n",
    "    for row in partition:\n",
    "        for feature in usable_features:\n",
    "            # Store feature-wise values in a dictionary\n",
    "            if feature not in feature_stats:\n",
    "                feature_stats[feature] = []\n",
    "\n",
    "            feature_stats[feature].append((row['features'][feature], row['label']))\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for feature, values in feature_stats.items():\n",
    "        results.append((feature, values))\n",
    "\n",
    "    return iter(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_split_feature` for finding splitting point with maximum variance reduction on the domain of a feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split_feature(values: list, parent_info: StatCounter):\n",
    "    # Sort values by feature value (ascending order)\n",
    "    sorted_values = sorted(values, key=lambda x: x[0])\n",
    "\n",
    "    # Get total number of data points\n",
    "    parent_count = parent_info.count()\n",
    "\n",
    "    # Get variance and mean of the whole dataset\n",
    "    parent_var = parent_info.variance()\n",
    "    parent_mean = parent_info.mean()\n",
    "\n",
    "    # Calculate parent's sum of squares: E[X²] = Var(X) + (E[X])²\n",
    "    parent_pow_sum = parent_var + parent_mean ** 2\n",
    "\n",
    "    # If not enough data to split, return no split\n",
    "    if parent_count < 2:\n",
    "        return (0, None)\n",
    "\n",
    "    # Initialize variables for left partition\n",
    "    left_sum, left_pow_sum, left_count = 0, 0, 0\n",
    "    best_split = (0, None)\n",
    "\n",
    "    # Iterate through possible split points\n",
    "    for i in range(parent_count - 1):\n",
    "        val, label = sorted_values[i]\n",
    "\n",
    "        # Accumulate stats for left partition\n",
    "        left_sum += label\n",
    "        left_pow_sum += label ** 2\n",
    "        left_count += 1\n",
    "\n",
    "        # Skip splitting between identical feature values\n",
    "        if val == sorted_values[i + 1][0]:\n",
    "            continue\n",
    "\n",
    "        # Compute right partition stats by subtracting left from parent\n",
    "        right_count = parent_count - left_count\n",
    "        right_sum = parent_info.sum() - left_sum\n",
    "        right_pow_sum = parent_pow_sum * parent_count - left_pow_sum  # Scale to total sum of squares\n",
    "\n",
    "        # Compute variance for each partition\n",
    "        left_var = (left_pow_sum / left_count - (left_sum / left_count) ** 2) if left_count > 0 else 0\n",
    "        right_var = (right_pow_sum / right_count - (right_sum / right_count) ** 2) if right_count > 0 else 0\n",
    "\n",
    "        # Calculate weighted variance reduction\n",
    "        var_reduction = parent_var - (left_var * left_count + right_var * right_count) / parent_count\n",
    "\n",
    "        # Update best split if this reduces variance more\n",
    "        if var_reduction > best_split[0]:\n",
    "            best_split = (var_reduction, (val + sorted_values[i + 1][0]) / 2)\n",
    "\n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-function `splitter` for splitting the rows of the parent dataset according to the splitting point and operands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(iterator, split_feature: int, split_point: float, operand):\n",
    "    ret = []\n",
    "    for row in iterator:\n",
    "        if operand(row['features'][split_feature], split_point):\n",
    "            ret.append(row)\n",
    "\n",
    "    return iter(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main class `DecisionTreeRegressor` for building and executing Decision Tree Regressor Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synchronized with maxDepth=10 from sections 3.2.1 and 3.2.2 (other parameters like maxBins and minInstancesPerNode cannot be directly applied in manual implementation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth = 1): # Synchronized with maxDepth=10 from sections 3.2.1 and 3.2.2\n",
    "        #Initialize the estimator with given depth (if any)\n",
    "        self.max_depth = max_depth\n",
    "        self.rules = None\n",
    "        self.num_features = None\n",
    "\n",
    "    def set_maxDepth(self, depth):\n",
    "        \"\"\"Set current maximum depth for the estimator.\"\"\"\n",
    "        self.max_depth = depth\n",
    "\n",
    "    def fit(self, train_rdd: RDD):\n",
    "        \"\"\"Execute Decision Tree Algorithm recursively on a given rdd based on variance reduction and the current maximum depth.\"\"\"\n",
    "        self.num_features = len(train_rdd.first()['features'])\n",
    "        self.usable_features = [i for i in range(self.num_features)]\n",
    "        sample_size = train_rdd.count()\n",
    "        print(f\"Starting tree construction with {sample_size} samples, max_depth={self.max_depth}\")\n",
    "        self.rules = self.__build_rule_tree_recursive(train_rdd, self.usable_features, sample_size)\n",
    "        print(\"Tree construction completed\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, rdd: RDD):\n",
    "        \"\"\"Make predictions on an RDD using the decision tree rules.\"\"\"\n",
    "        def predict_row(row):\n",
    "            # Start from the root of the decision tree\n",
    "            node = self.rules\n",
    "\n",
    "            # Traverse the tree until reaching a leaf node (which contains a prediction)\n",
    "            while 'prediction' not in node:\n",
    "                # Compare the feature value with the split point to decide the direction\n",
    "                if row['features'][node['split_feature']] <= node['split_point']:\n",
    "                    node = node['left']  # Go to the left subtree\n",
    "                else:\n",
    "                    node = node['right']  # Go to the right subtree\n",
    "\n",
    "            # Return a tuple of (row ID, predicted value)\n",
    "            return (row['id'], node['prediction'])\n",
    "\n",
    "        # Apply prediction to each row in the RDD\n",
    "        return rdd.map(lambda row: predict_row(row))\n",
    "\n",
    "    def display_rule_tree(self):\n",
    "        \"\"\"Recursively display the rules of a Decision Tree model.\"\"\"\n",
    "        print(\"\\nDecision Tree Rules:\")\n",
    "        self.__display_tree_recursive(self.rules)\n",
    "\n",
    "    def __display_tree_recursive(self, rules: dict, indent = 0):\n",
    "        #Stopping condition\n",
    "        if not rules:\n",
    "            return\n",
    "\n",
    "        if 'prediction' in rules:\n",
    "            #Is a leaf condition\n",
    "            print(f\"{indent * ' '}Predict: {rules['prediction']:.2f}\")\n",
    "            return\n",
    "\n",
    "        #Print the splitting point information and call recursion of the left and right child\n",
    "\n",
    "        print(f\"{indent * ' '}If feature[{rules['split_feature']}] <= {rules['split_point']:.2f}\")\n",
    "        self.__display_tree_recursive(rules['left'], indent + 4)\n",
    "        print(f\"{indent * ' '}Else feature[{rules['split_feature']}] > {rules['split_point']:.2f}\")\n",
    "        self.__display_tree_recursive(rules['right'], indent + 4)       \n",
    "\n",
    "    def __find_best_split(self, rdd: RDD, usable_features: list):         \n",
    "        \"\"\"Find the best splitting point with maximum variance reduction of the input dataset.\"\"\"\n",
    "\n",
    "        #Compute the dataset statistics and store into a StatCounter object\n",
    "        parent_info = rdd.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2))\n",
    "        \n",
    "        #Convert each row into a list of (row_feature_value, row_label) and flatten the results\n",
    "        processed_rdd = rdd.mapPartitions(lambda partition: partition_feature_grouping(partition, usable_features), True)\\\n",
    "                      .reduceByKey(lambda l1, l2: l1 + l2).cache()\n",
    "        \n",
    "        #Find the best splitting point for each features\n",
    "        best_split_per_feature = processed_rdd.mapValues(lambda values: find_split_feature(values, parent_info))\\\n",
    "                                  \n",
    "        #Find the best splitting point for the dataset\n",
    "        best_split = max(best_split_per_feature.collect(),key=lambda x: (x[1][0], -x[0]))\n",
    "\n",
    "        return best_split[0], best_split[1][1]\n",
    "\n",
    "\n",
    "    def __build_rule_tree_recursive(self, parent: RDD, usable_features: list, sample_size, depth = 0):\n",
    "        \"\"\"Recursively build the decision tree by finding the splitting point with maximum variance reduction and split the dataset with this point, up to the maximum depth.\"\"\"\n",
    "        print(f\"Building tree at depth {depth}, sample size: {sample_size}\")\n",
    "        #Stopping condition\n",
    "        if sample_size == 0:\n",
    "            return None\n",
    "        \n",
    "        #Return the mean of un-splitted subset as the prediction if reached the depth limit\n",
    "        if depth == self.max_depth or sample_size < 2:\n",
    "            mean = parent.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2)).mean()\n",
    "\n",
    "            return {'prediction' : mean}\n",
    "\n",
    "        #Find the best splitting point for the input dataset\n",
    "        split_feature, split_point = self.__find_best_split(parent, usable_features)\n",
    "        \n",
    "        #Return the mean of un-splitted subset as the prediction if no splitting point is valid but has not reached the depth limit yet\n",
    "        if split_point == None:\n",
    "            mean = parent.mapPartitions(lambda partition: [StatCounter([row['label'] for row in partition])], True)\\\n",
    "                        .reduce(lambda stat1, stat2: stat1.mergeStats(stat2)).mean()\n",
    "            \n",
    "            return {'prediction' : mean}\n",
    "\n",
    "        #Split the dataset into left and right subsets\n",
    "        left_rdd = parent.mapPartitions(lambda iterator: splitter(iterator, split_feature, split_point, lambda a,b: a <= b), True).cache()\n",
    "        right_rdd = parent.mapPartitions(lambda iterator: splitter(iterator, split_feature, split_point, lambda a,b: a >= b), True).cache()\n",
    "        \n",
    "        #Un-cache the parent dataset (excluding the input one)\n",
    "        if depth > 0:\n",
    "            parent.unpersist()\n",
    "\n",
    "        #Compute the size of the left subset\n",
    "        left_sample_size = left_rdd.count()\n",
    "     \n",
    "        #Build the dict of information with recursion call\n",
    "        return {\n",
    "            'split_feature' : split_feature,\n",
    "            'split_point' : split_point,\n",
    "            'left' : self.__build_rule_tree_recursive(left_rdd, usable_features, left_sample_size, depth + 1),\n",
    "            'right' : self.__build_rule_tree_recursive(right_rdd, usable_features, sample_size - left_sample_size, depth + 1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 23:34:50 WARN BlockManager: Task 2 already completed, not releasing lock for rdd_6_0\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tree construction with 1458644 samples, max_depth=10\n",
      "Building tree at depth 0, sample size: 1458644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 1, sample size: 1160867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 2, sample size: 677778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 3, sample size: 356233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 166340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 90855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 6, sample size: 69565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 12591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 366\n",
      "Building tree at depth 10, sample size: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 204\n",
      "Building tree at depth 10, sample size: 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 11869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 11855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 768\n",
      "Building tree at depth 10, sample size: 11087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 13\n",
      "Building tree at depth 10, sample size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 56974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 32794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 2704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 1357\n",
      "Building tree at depth 10, sample size: 1347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 30090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 1502\n",
      "Building tree at depth 10, sample size: 28588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 24180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 24173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 18830\n",
      "Building tree at depth 10, sample size: 5343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 3\n",
      "Building tree at depth 10, sample size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 6, sample size: 21290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 1768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 386\n",
      "Building tree at depth 10, sample size: 113\n",
      "Building tree at depth 9, sample size: 328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 273\n",
      "Building tree at depth 10, sample size: 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 907\n",
      "Building tree at depth 10, sample size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 1\n",
      "Building tree at depth 10, sample size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 19522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 9696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 630\n",
      "Building tree at depth 10, sample size: 599\n",
      "Building tree at depth 10, sample size: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 9066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 6430\n",
      "Building tree at depth 10, sample size: 2636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 9826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 3231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 3219\n",
      "Building tree at depth 10, sample size: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 6595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 494\n",
      "Building tree at depth 10, sample size: 6101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 75485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 6, sample size: 58843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 9576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 8085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 5799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 2962\n",
      "Building tree at depth 10, sample size: 2837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 2286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 746\n",
      "Building tree at depth 10, sample size: 1540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 1491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 382\n",
      "Building tree at depth 10, sample size: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 1103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 951\n",
      "Building tree at depth 10, sample size: 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 49267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 49250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 37842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 14793\n",
      "Building tree at depth 10, sample size: 23049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 11408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 4688\n",
      "Building tree at depth 10, sample size: 6720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 1\n",
      "Building tree at depth 9, sample size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 15\n",
      "Building tree at depth 10, sample size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 6, sample size: 16642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 14230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 1065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 1035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 827\n",
      "Building tree at depth 10, sample size: 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 7\n",
      "Building tree at depth 10, sample size: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 13165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 4638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 1694\n",
      "Building tree at depth 10, sample size: 2944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 8527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 3166\n",
      "Building tree at depth 10, sample size: 5361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 2412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 2405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 1413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 298\n",
      "Building tree at depth 10, sample size: 1115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 146\n",
      "Building tree at depth 10, sample size: 846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 2\n",
      "Building tree at depth 10, sample size: 1\n",
      "Building tree at depth 10, sample size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 3\n",
      "Building tree at depth 10, sample size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 189893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 143087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 6, sample size: 143072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 58771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 4487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 4483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 19\n",
      "Building tree at depth 10, sample size: 4464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 1\n",
      "Building tree at depth 10, sample size: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 54284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 31925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 31873\n",
      "Building tree at depth 10, sample size: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 22359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 3308\n",
      "Building tree at depth 10, sample size: 19051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 84301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 1\n",
      "Building tree at depth 8, sample size: 84300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 10943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 929\n",
      "Building tree at depth 10, sample size: 10014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 73357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 14\n",
      "Building tree at depth 10, sample size: 73343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 6, sample size: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 2\n",
      "Building tree at depth 10, sample size: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 1\n",
      "Building tree at depth 10, sample size: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 46806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 6, sample size: 44814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 2233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 2232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 65\n",
      "Building tree at depth 10, sample size: 266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 1901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 301\n",
      "Building tree at depth 10, sample size: 1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 42581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 34531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 39\n",
      "Building tree at depth 10, sample size: 1951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 32541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 24798\n",
      "Building tree at depth 10, sample size: 7743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 8050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 1\n",
      "Building tree at depth 10, sample size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 8048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 146\n",
      "Building tree at depth 10, sample size: 7902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 6, sample size: 1992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 1\n",
      "Building tree at depth 9, sample size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 1\n",
      "Building tree at depth 9, sample size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 1988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 1\n",
      "Building tree at depth 10, sample size: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 1977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 19\n",
      "Building tree at depth 10, sample size: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 1862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 172\n",
      "Building tree at depth 10, sample size: 1690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 3, sample size: 321545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 4, sample size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 1\n",
      "Building tree at depth 5, sample size: 1\n",
      "Building tree at depth 4, sample size: 321543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 150066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 6, sample size: 92052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 74656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 12230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 9219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 3924\n",
      "Building tree at depth 10, sample size: 5295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 3011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 1605\n",
      "Building tree at depth 10, sample size: 1406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 62426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 42639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 15454\n",
      "Building tree at depth 10, sample size: 27185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 19787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 10856\n",
      "Building tree at depth 10, sample size: 8931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 17396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 14568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 810\n",
      "Building tree at depth 10, sample size: 264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 13494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 6158\n",
      "Building tree at depth 10, sample size: 7336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 2828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 253\n",
      "Building tree at depth 10, sample size: 574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 2001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 1708\n",
      "Building tree at depth 10, sample size: 293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 6, sample size: 58014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 1\n",
      "Building tree at depth 10, sample size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 64\n",
      "Building tree at depth 10, sample size: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 57930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 8995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 6743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 2468\n",
      "Building tree at depth 10, sample size: 4275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 2252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 1\n",
      "Building tree at depth 10, sample size: 2251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 48935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 37497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 25116\n",
      "Building tree at depth 10, sample size: 12381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 11438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 9173\n",
      "Building tree at depth 10, sample size: 2265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 5, sample size: 171477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 6, sample size: 133343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 20780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 9224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 9219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 9135\n",
      "Building tree at depth 10, sample size: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 4\n",
      "Building tree at depth 10, sample size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 11556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 9646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 341\n",
      "Building tree at depth 10, sample size: 9305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 1878\n",
      "Building tree at depth 10, sample size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 7, sample size: 112563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 49194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 49178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 21012\n",
      "Building tree at depth 10, sample size: 28166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 14\n",
      "Building tree at depth 10, sample size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 8, sample size: 63369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 9, sample size: 1\n",
      "Building tree at depth 9, sample size: 63368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 10, sample size: 30081\n",
      "Building tree at depth 10, sample size: 33287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree at depth 6, sample size: 38134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "25/04/11 23:47:04 WARN BlockManager: Putting block rdd_1545_5 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1194, in main\n",
      "    importlib.invalidate_caches()\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/importlib/__init__.py\", line 70, in invalidate_caches\n",
      "    finder.invalidate_caches()\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1403, in invalidate_caches\n",
      "  File \"<frozen zipimport>\", line 332, in invalidate_caches\n",
      "  File \"<frozen zipimport>\", line 484, in _read_directory\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 92, in _unpack_uint16\n",
      "KeyboardInterrupt\n",
      ".\n",
      "25/04/11 23:47:04 WARN BlockManager: Block rdd_1545_5 could not be removed as it was not found on disk or in memory\n",
      "25/04/11 23:47:04 WARN BlockManager: Task 4622 already completed, not releasing lock for rdd_6_5\n",
      "25/04/11 23:47:04 WARN BlockManager: Putting block rdd_1545_1 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_85639/3719947832.py\", line 109, in <lambda>\n",
      "  File \"/tmp/ipykernel_85639/1767614009.py\", line 3, in splitter\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      ".\n",
      "25/04/11 23:47:04 WARN BlockManager: Block rdd_1545_1 could not be removed as it was not found on disk or in memory\n",
      "25/04/11 23:47:04 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1194, in main\n",
      "    importlib.invalidate_caches()\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/importlib/__init__.py\", line 70, in invalidate_caches\n",
      "    finder.invalidate_caches()\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1403, in invalidate_caches\n",
      "  File \"<frozen zipimport>\", line 332, in invalidate_caches\n",
      "  File \"<frozen zipimport>\", line 482, in _read_directory\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 89, in _unpack_uint16\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1194, in main\n",
      "    importlib.invalidate_caches()\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/importlib/__init__.py\", line 70, in invalidate_caches\n",
      "    finder.invalidate_caches()\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1403, in invalidate_caches\n",
      "  File \"<frozen zipimport>\", line 332, in invalidate_caches\n",
      "  File \"<frozen zipimport>\", line 501, in _read_directory\n",
      "KeyboardInterrupt\n",
      "\n",
      "\t... 13 more\n",
      "25/04/11 23:47:04 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1614)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/04/11 23:47:04 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\t... 13 more\n",
      "25/04/11 23:47:04 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/04/11 23:47:04 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1194, in main\n",
      "    importlib.invalidate_caches()\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/importlib/__init__.py\", line 70, in invalidate_caches\n",
      "    finder.invalidate_caches()\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1403, in invalidate_caches\n",
      "  File \"<frozen zipimport>\", line 332, in invalidate_caches\n",
      "  File \"<frozen zipimport>\", line 501, in _read_directory\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/04/11 23:47:04 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/04/11 23:47:04 WARN BlockManager: Putting block rdd_1545_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      ".\n",
      "25/04/11 23:47:04 WARN BlockManager: Block rdd_1545_0 could not be removed as it was not found on disk or in memory\n",
      "25/04/11 23:47:04 ERROR Executor: Exception in task 1.0 in stage 772.0 (TID 4618)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_85639/3719947832.py\", line 109, in <lambda>\n",
      "  File \"/tmp/ipykernel_85639/1767614009.py\", line 3, in splitter\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/04/11 23:47:04 ERROR Executor: Exception in task 0.0 in stage 772.0 (TID 4617)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/04/11 23:47:04 WARN BlockManager: Putting block rdd_1545_2 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_85639/3719947832.py\", line 109, in <lambda>\n",
      "  File \"/tmp/ipykernel_85639/1767614009.py\", line 3, in splitter\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      ".\n",
      "25/04/11 23:47:04 WARN BlockManager: Block rdd_1545_2 could not be removed as it was not found on disk or in memory\n",
      "25/04/11 23:47:04 WARN BlockManager: Putting block rdd_1545_3 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      ".\n",
      "25/04/11 23:47:04 WARN BlockManager: Block rdd_1545_3 could not be removed as it was not found on disk or in memory\n",
      "25/04/11 23:47:04 ERROR Executor: Exception in task 3.0 in stage 772.0 (TID 4620)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1614)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/11 23:47:04 WARN TaskSetManager: Lost task 1.0 in stage 772.0 (TID 4618) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_85639/3719947832.py\", line 109, in <lambda>\n",
      "  File \"/tmp/ipykernel_85639/1767614009.py\", line 3, in splitter\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "25/04/11 23:47:04 ERROR Executor: Exception in task 5.0 in stage 772.0 (TID 4622)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1194, in main\n",
      "    importlib.invalidate_caches()\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/importlib/__init__.py\", line 70, in invalidate_caches\n",
      "    finder.invalidate_caches()\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1403, in invalidate_caches\n",
      "  File \"<frozen zipimport>\", line 332, in invalidate_caches\n",
      "  File \"<frozen zipimport>\", line 484, in _read_directory\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 92, in _unpack_uint16\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/04/11 23:47:04 ERROR TaskSetManager: Task 1 in stage 772.0 failed 1 times; aborting job\n",
      "25/04/11 23:47:04 ERROR Executor: Exception in task 2.0 in stage 772.0 (TID 4619)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_85639/3719947832.py\", line 109, in <lambda>\n",
      "  File \"/tmp/ipykernel_85639/1767614009.py\", line 3, in splitter\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/04/11 23:47:04 WARN TaskSetManager: Lost task 3.0 in stage 772.0 (TID 4620) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1614)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/04/11 23:47:04 WARN BlockManager: Putting block rdd_1545_4 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_85639/3719947832.py\", line 109, in <lambda>\n",
      "  File \"/tmp/ipykernel_85639/1767614009.py\", line 3, in splitter\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 166, in _read_with_length\n",
      "    length = read_int(stream)\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      ".\n",
      "25/04/11 23:47:04 WARN BlockManager: Block rdd_1545_4 could not be removed as it was not found on disk or in memory\n",
      "25/04/11 23:47:04 ERROR Executor: Exception in task 4.0 in stage 772.0 (TID 4621)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_85639/3719947832.py\", line 109, in <lambda>\n",
      "  File \"/tmp/ipykernel_85639/1767614009.py\", line 3, in splitter\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 166, in _read_with_length\n",
      "    length = read_int(stream)\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/04/11 23:47:04 WARN TaskSetManager: Lost task 0.0 in stage 772.0 (TID 4617) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "25/04/11 23:47:04 WARN TaskSetManager: Lost task 5.0 in stage 772.0 (TID 4622) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1194, in main\n",
      "    importlib.invalidate_caches()\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/importlib/__init__.py\", line 70, in invalidate_caches\n",
      "    finder.invalidate_caches()\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1403, in invalidate_caches\n",
      "  File \"<frozen zipimport>\", line 332, in invalidate_caches\n",
      "  File \"<frozen zipimport>\", line 484, in _read_directory\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 92, in _unpack_uint16\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "25/04/11 23:47:04 WARN TaskSetManager: Lost task 4.0 in stage 772.0 (TID 4621) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_85639/3719947832.py\", line 109, in <lambda>\n",
      "  File \"/tmp/ipykernel_85639/1767614009.py\", line 3, in splitter\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 166, in _read_with_length\n",
      "    length = read_int(stream)\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hadoop/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training with max_depth=10\u001b[39;00m\n\u001b[1;32m      2\u001b[0m estimator \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# Synchronized with maxDepth=10 from sections 3.2.1 and 3.2.2\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mfit(train_rdd)\n\u001b[1;32m      4\u001b[0m estimator\u001b[38;5;241m.\u001b[39mdisplay_rule_tree()\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, train_rdd)\u001b[0m\n\u001b[1;32m     16\u001b[0m sample_size \u001b[38;5;241m=\u001b[39m train_rdd\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting tree construction with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples, max_depth=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(train_rdd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musable_features, sample_size)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTree construction completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[10], line 123\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.__build_rule_tree_recursive\u001b[0;34m(self, parent, usable_features, sample_size, depth)\u001b[0m\n\u001b[1;32m    117\u001b[0m left_sample_size \u001b[38;5;241m=\u001b[39m left_rdd\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#Build the dict of information with recursion call\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_feature\u001b[39m\u001b[38;5;124m'\u001b[39m : split_feature,\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_point\u001b[39m\u001b[38;5;124m'\u001b[39m : split_point,\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(left_rdd, usable_features, left_sample_size, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(right_rdd, usable_features, sample_size \u001b[38;5;241m-\u001b[39m left_sample_size, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    125\u001b[0m }\n",
      "Cell \u001b[0;32mIn[10], line 123\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.__build_rule_tree_recursive\u001b[0;34m(self, parent, usable_features, sample_size, depth)\u001b[0m\n\u001b[1;32m    117\u001b[0m left_sample_size \u001b[38;5;241m=\u001b[39m left_rdd\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#Build the dict of information with recursion call\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_feature\u001b[39m\u001b[38;5;124m'\u001b[39m : split_feature,\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_point\u001b[39m\u001b[38;5;124m'\u001b[39m : split_point,\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(left_rdd, usable_features, left_sample_size, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(right_rdd, usable_features, sample_size \u001b[38;5;241m-\u001b[39m left_sample_size, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    125\u001b[0m }\n",
      "Cell \u001b[0;32mIn[10], line 124\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.__build_rule_tree_recursive\u001b[0;34m(self, parent, usable_features, sample_size, depth)\u001b[0m\n\u001b[1;32m    117\u001b[0m left_sample_size \u001b[38;5;241m=\u001b[39m left_rdd\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#Build the dict of information with recursion call\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_feature\u001b[39m\u001b[38;5;124m'\u001b[39m : split_feature,\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_point\u001b[39m\u001b[38;5;124m'\u001b[39m : split_point,\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(left_rdd, usable_features, left_sample_size, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(right_rdd, usable_features, sample_size \u001b[38;5;241m-\u001b[39m left_sample_size, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    125\u001b[0m }\n",
      "Cell \u001b[0;32mIn[10], line 124\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.__build_rule_tree_recursive\u001b[0;34m(self, parent, usable_features, sample_size, depth)\u001b[0m\n\u001b[1;32m    117\u001b[0m left_sample_size \u001b[38;5;241m=\u001b[39m left_rdd\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#Build the dict of information with recursion call\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_feature\u001b[39m\u001b[38;5;124m'\u001b[39m : split_feature,\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_point\u001b[39m\u001b[38;5;124m'\u001b[39m : split_point,\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(left_rdd, usable_features, left_sample_size, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(right_rdd, usable_features, sample_size \u001b[38;5;241m-\u001b[39m left_sample_size, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    125\u001b[0m }\n",
      "    \u001b[0;31m[... skipping similar frames: DecisionTreeRegressor.__build_rule_tree_recursive at line 124 (1 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 124\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.__build_rule_tree_recursive\u001b[0;34m(self, parent, usable_features, sample_size, depth)\u001b[0m\n\u001b[1;32m    117\u001b[0m left_sample_size \u001b[38;5;241m=\u001b[39m left_rdd\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#Build the dict of information with recursion call\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_feature\u001b[39m\u001b[38;5;124m'\u001b[39m : split_feature,\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_point\u001b[39m\u001b[38;5;124m'\u001b[39m : split_point,\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(left_rdd, usable_features, left_sample_size, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_rule_tree_recursive(right_rdd, usable_features, sample_size \u001b[38;5;241m-\u001b[39m left_sample_size, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    125\u001b[0m }\n",
      "Cell \u001b[0;32mIn[10], line 99\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.__build_rule_tree_recursive\u001b[0;34m(self, parent, usable_features, sample_size, depth)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m : mean}\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m#Find the best splitting point for the input dataset\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m split_feature, split_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__find_best_split(parent, usable_features)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m#Return the mean of un-splitted subset as the prediction if no splitting point is valid but has not reached the depth limit yet\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_point \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[10], line 69\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.__find_best_split\u001b[0;34m(self, rdd, usable_features)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Find the best splitting point with maximum variance reduction of the input dataset.\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#Compute the dataset statistics and store into a StatCounter object\u001b[39;00m\n\u001b[1;32m     68\u001b[0m parent_info \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(\u001b[38;5;28;01mlambda\u001b[39;00m partition: [StatCounter([row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m partition])], \u001b[38;5;28;01mTrue\u001b[39;00m)\\\n\u001b[0;32m---> 69\u001b[0m                 \u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28;01mlambda\u001b[39;00m stat1, stat2: stat1\u001b[38;5;241m.\u001b[39mmergeStats(stat2))\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#Convert each row into a list of (row_feature_value, row_label) and flatten the results\u001b[39;00m\n\u001b[1;32m     72\u001b[0m processed_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(\u001b[38;5;28;01mlambda\u001b[39;00m partition: partition_feature_grouping(partition, usable_features), \u001b[38;5;28;01mTrue\u001b[39;00m)\\\n\u001b[1;32m     73\u001b[0m               \u001b[38;5;241m.\u001b[39mreduceByKey(\u001b[38;5;28;01mlambda\u001b[39;00m l1, l2: l1 \u001b[38;5;241m+\u001b[39m l2)\u001b[38;5;241m.\u001b[39mcache()\n",
      "File \u001b[0;32m~/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/rdd.py:1924\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m-> 1924\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapPartitions(func)\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[0;32m~/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mcollectAndServe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd\u001b[38;5;241m.\u001b[39mrdd())\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/miniconda3/envs/mrjob_env/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mrjob_env/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training with max_depth=10\n",
    "estimator = DecisionTreeRegressor(max_depth=10)  # Synchronized with maxDepth=10 from sections 3.2.1 and 3.2.2\n",
    "model = estimator.fit(train_rdd)\n",
    "estimator.display_rule_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample predictions for a few test cases (test file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 22:25:39 WARN BlockManager: Task 45 already completed, not releasing lock for rdd_7_0\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: id3004672, Prediction: 735.6992713204887\n",
      "ID: id3505355, Prediction: 735.6992713204887\n",
      "ID: id1217141, Prediction: 735.6992713204887\n",
      "ID: id2150126, Prediction: 1831.9367882677282\n",
      "ID: id1598245, Prediction: 735.6992713204887\n"
     ]
    }
   ],
   "source": [
    "# Predict and display samples\n",
    "predictions_rdd = estimator.transform(test_rdd)\n",
    "print(\"Sample Predictions:\")\n",
    "predictions_samples = predictions_rdd.take(5)\n",
    "for pred in predictions_samples:\n",
    "    print(f\"ID: {pred[0]}, Prediction: {pred[1]}\")\n",
    "\n",
    "# Stop SparkContext\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrjob_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
