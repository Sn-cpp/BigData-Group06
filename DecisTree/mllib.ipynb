{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor using Pyspark MLLib's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.feature import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import col, udf\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize a Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/04/11 20:12:29 WARN Utils: Your hostname, HP-Envy resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/11 20:12:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/11 20:12:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "builder = SparkSession.Builder().appName('taxi_duration_mllib')\n",
    "spark = builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read input datasets for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Read input files\n",
    "raw_train_data = spark.read.csv('train.csv', header=True, inferSchema=True)\n",
    "raw_test_data = spark.read.csv('test.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse timestamp features in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cast the pickup string values of training data into timestamps.\n",
    "casted_train_data = raw_train_data.withColumns({\n",
    "                        'pickup_datetime' : raw_train_data['pickup_datetime'].cast('timestamp'),\n",
    "                    })\n",
    "\n",
    "#Cast the pickup string values of testing data into timestamps.\n",
    "casted_test_data = raw_test_data.withColumns({\n",
    "                        'pickup_datetime' : raw_test_data['pickup_datetime'].cast('timestamp')\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-Defined Function (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm tính khoảng cách Haversine\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    if None in (lon1, lat1, lon2, lat2) or not all(isinstance(x, (int, float)) for x in [lon1, lat1, lon2, lat2]):\n",
    "        return 0.0\n",
    "    R = 6371  # Bán kính Trái Đất (km)\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "haversine_udf = udf(haversine, DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract usable features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get usable columns from the dataframe\n",
    "#Also convert timestamps into time elements and encode `store_and_fwd_flag` feature into binary values\n",
    "extracted_train_df = casted_train_data.selectExpr(\n",
    "        'id',\n",
    "        'vendor_id',\n",
    "        'YEAR(pickup_datetime)    AS pickup_year',\n",
    "        'MONTH(pickup_datetime)   AS pickup_month',\n",
    "        'DAY(pickup_datetime)     AS pickup_day',\n",
    "        'HOUR(pickup_datetime)    AS pickup_hour',\n",
    "        'MINUTE(pickup_datetime)  AS pickup_min',\n",
    "        'SECOND(pickup_datetime)  AS pickup_sec',\n",
    "        'passenger_count', \n",
    "        'pickup_longitude', \n",
    "        'pickup_latitude', \n",
    "        'dropoff_longitude', \n",
    "        'dropoff_latitude',\n",
    "        'CASE WHEN store_and_fwd_flag == \"Y\" THEN 1 ELSE 0 END AS store_and_fwd_flag',\n",
    "        'trip_duration'\n",
    "    ).withColumn('distance_km', haversine_udf(col('pickup_longitude'), col('pickup_latitude'), col('dropoff_longitude'), col('dropoff_latitude')))\n",
    "\n",
    "extracted_test_df = casted_test_data.selectExpr(\n",
    "        'id',\n",
    "        'vendor_id',\n",
    "        'YEAR(pickup_datetime)    AS pickup_year',\n",
    "        'MONTH(pickup_datetime)   AS pickup_month',\n",
    "        'DAY(pickup_datetime)     AS pickup_day',\n",
    "        'HOUR(pickup_datetime)    AS pickup_hour',\n",
    "        'MINUTE(pickup_datetime)  AS pickup_min',\n",
    "        'SECOND(pickup_datetime)  AS pickup_sec',\n",
    "        'passenger_count', \n",
    "        'pickup_longitude', \n",
    "        'pickup_latitude', \n",
    "        'dropoff_longitude', \n",
    "        'dropoff_latitude',\n",
    "        'CASE WHEN store_and_fwd_flag == \"Y\" THEN 1 ELSE 0 END AS store_and_fwd_flag',\n",
    "    ).withColumn('distance_km', haversine_udf(col('pickup_longitude'), col('pickup_latitude'), col('dropoff_longitude'), col('dropoff_latitude')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiểm tra giá trị null trong extracted_train_df:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 0 giá trị null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vendor_id: 0 giá trị null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_year: 0 giá trị null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_month: 0 giá trị null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_day: 0 giá trị null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_hour: 0 giá trị null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_min: 0 giá trị null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_sec: 0 giá trị null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passenger_count: 0 giá trị null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_longitude: 0 giá trị null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_latitude: 0 giá trị null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropoff_longitude: 0 giá trị null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropoff_latitude: 0 giá trị null\n",
      "store_and_fwd_flag: 0 giá trị null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_duration: 0 giá trị null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance_km: 0 giá trị null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Loại bỏ outliers\n",
    "extracted_train_df = extracted_train_df.filter(\n",
    "    (col('trip_duration') > 0) & \n",
    "    (col('trip_duration') < 36000) &  # Giới hạn 10 giờ\n",
    "    (col('passenger_count') >= 1) & \n",
    "    (col('passenger_count') <= 6) &  # Giới hạn hành khách\n",
    "    (col('distance_km') > 0) & \n",
    "    (col('distance_km') < 100)  # Giới hạn khoảng cách 100km\n",
    ")\n",
    "\n",
    "# Kiểm tra giá trị null\n",
    "print(\"Kiểm tra giá trị null trong extracted_train_df:\")\n",
    "for column in extracted_train_df.columns:\n",
    "    null_count = extracted_train_df.filter(col(column).isNull()).count()\n",
    "    print(f\"{column}: {null_count} giá trị null\")\n",
    "\n",
    "extracted_train_df = extracted_train_df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, convert the dataset into a RDD of `LabeledPoint` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh sách đặc trưng (giống 3.2.1)\n",
    "feature_cols = ['vendor_id', 'pickup_year', 'pickup_month', 'pickup_day', 'pickup_hour',\n",
    "                'pickup_min', 'pickup_sec', 'passenger_count', 'pickup_longitude',\n",
    "                'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', \n",
    "                'store_and_fwd_flag', 'distance_km']\n",
    "\n",
    "#Convert to rdd\n",
    "# train_data = extracted_train_df.rdd.map(lambda row: LabeledPoint(label= row[-1], features= row[:-1]) )\n",
    "# test_data = extracted_test_df.rdd.map(lambda row: LabeledPoint(label= float('-inf'), features= row[:-1]) )\n",
    "def to_labeled_point(row):\n",
    "    features = [float(row[col]) for col in feature_cols]\n",
    "    # Ánh xạ vendor_id: 1 -> 0, 2 -> 1\n",
    "    features[0] = features[0] - 1  # vendor_id ở index 0\n",
    "    return LabeledPoint(row['trip_duration'], features)\n",
    "\n",
    "train_data = extracted_train_df.rdd.map(to_labeled_point)\n",
    "\n",
    "# Chuyển đổi tập test\n",
    "def to_features_test(row):\n",
    "    features = [float(row[col]) for col in feature_cols]\n",
    "    features[0] = features[0] - 1  # Ánh xạ vendor_id: 1 -> 0, 2 -> 1\n",
    "    return features\n",
    "\n",
    "test_features = extracted_test_df.rdd.map(to_features_test)\n",
    "test_ids = extracted_test_df.rdd.map(lambda row: row['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[197] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "# Training set proportion parameter:\n",
    "train_size = 0.8\n",
    "\n",
    "train_rdd, validation_rdd = train_data.randomSplit([train_size, 1 - train_size], seed=24) #Fixed with seed for reproductivity\n",
    "\n",
    "# Cache RDD (Lưu train_rdd và validation_rdd vào bộ nhớ để giảm tải)\n",
    "train_rdd.cache()\n",
    "validation_rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 20:13:26 WARN BlockManager: Task 290 already completed, not releasing lock for rdd_196_0\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# model = DecisionTree.trainRegressor(train, categoricalFeaturesInfo={})\n",
    "\n",
    "# Sử dụng tham số tối ưu từ 3.2.1\n",
    "best_params = {'maxDepth': 10, 'maxBins': 64, 'minInstancesPerNode': 5}\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model = DecisionTree.trainRegressor(\n",
    "    train_rdd,\n",
    "    categoricalFeaturesInfo={0: 2, 7: 10, 12: 2},  # vendor_id, passenger_count, store_and_fwd_flag\n",
    "    impurity='variance',\n",
    "    maxDepth=best_params['maxDepth'],\n",
    "    maxBins=best_params['maxBins'],\n",
    "    minInstancesPerNode=best_params['minInstancesPerNode']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = model.predict(test.map(lambda row: row.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_pred = test.map(lambda row: row.label).zip(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_pred.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model evaluation (hold-out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatake/.local/lib/python3.10/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "[Stage 77:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE trên validation (MLlib RDD): 394.66337924115203\n",
      "R2 trên validation (MLlib RDD): 0.5075514363493248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Đánh giá trên validation\n",
    "predictions = model.predict(validation_rdd.map(lambda lp: lp.features))\n",
    "labels_and_preds = validation_rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "metrics = RegressionMetrics(labels_and_preds)\n",
    "rmse = metrics.rootMeanSquaredError\n",
    "r2 = metrics.r2\n",
    "\n",
    "print(f\"RMSE trên validation (MLlib RDD): {rmse}\")\n",
    "print(f\"R2 trên validation (MLlib RDD): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison with Structured API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured API RMSE: 382.92100639699294\n",
      "Structured API R2: 0.68297963107283\n",
      "Chênh lệch RMSE: 11.742372844159092\n",
      "Chênh lệch R2: -0.17542819472350524\n"
     ]
    }
   ],
   "source": [
    "structured_api_rmse = 382.92100639699294 # Số từ 3.2.1\n",
    "structured_api_r2 =  0.68297963107283  # Số từ 3.2.1\n",
    "print(f\"Structured API RMSE: {structured_api_rmse}\")\n",
    "print(f\"Structured API R2: {structured_api_r2}\")\n",
    "print(f\"Chênh lệch RMSE: {rmse - structured_api_rmse}\")\n",
    "print(f\"Chênh lệch R2: {r2 - structured_api_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction (test file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|       id|     trip_duration|\n",
      "+---------+------------------+\n",
      "|id3004672| 579.4204229607251|\n",
      "|id3505355| 622.9761684987833|\n",
      "|id1217141|473.06607351225205|\n",
      "|id2150126|1273.8461140566403|\n",
      "|id1598245| 343.8123226591001|\n",
      "+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dự đoán trên test\n",
    "test_predictions = model.predict(test_features)\n",
    "test_predictions_df = spark.createDataFrame(\n",
    "    test_ids.zip(test_predictions),\n",
    "    schema=['id', 'trip_duration']\n",
    ")\n",
    "\n",
    "# Write file\n",
    "test_predictions_df.coalesce(1).write.csv(\"prediction_mllib.csv\", header=True, mode='overwrite')\n",
    "\n",
    "test_predictions_df.show(5)\n",
    "\n",
    "# Đóng SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
