{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor using Pyspark MLLib's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.feature import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.sql.functions import col, udf\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize a Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/04/12 10:02:44 WARN Utils: Your hostname, HP-Envy resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/12 10:02:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/12 10:02:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#pyspark init\n",
    "builder = SparkSession.builder\\\n",
    "            .appName('taxi_duration_mllib')\\\n",
    "            .config(\"spark.driver.memory\", \"4g\")\\\n",
    "            .config(\"spark.executor.memory\", \"4g\")\n",
    "spark = builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read input datasets for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Read input files\n",
    "raw_train_data = spark.read.csv('train.csv', header=True, inferSchema=True)\n",
    "raw_test_data = spark.read.csv('test.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse timestamp features in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cast the pickup string values of training data into timestamps.\n",
    "casted_train_data = raw_train_data.withColumns({\n",
    "                        'pickup_datetime' : raw_train_data['pickup_datetime'].cast('timestamp'),\n",
    "                    })\n",
    "\n",
    "#Cast the pickup string values of testing data into timestamps.\n",
    "casted_test_data = raw_test_data.withColumns({\n",
    "                        'pickup_datetime' : raw_test_data['pickup_datetime'].cast('timestamp')\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-Defined Function (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haversine distance function\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    if None in (lon1, lat1, lon2, lat2) or not all(isinstance(x, (int, float)) for x in [lon1, lat1, lon2, lat2]):\n",
    "        return 0.0\n",
    "    R = 6371  # Earth Radius (km)\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "haversine_udf = udf(haversine, DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract usable features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get usable columns from the dataframe\n",
    "#Also convert timestamps into time elements and encode `store_and_fwd_flag` feature into binary values\n",
    "extracted_train_df = casted_train_data.selectExpr(\n",
    "        'id',\n",
    "        'vendor_id',\n",
    "        'YEAR(pickup_datetime)    AS pickup_year',\n",
    "        'MONTH(pickup_datetime)   AS pickup_month',\n",
    "        'DAY(pickup_datetime)     AS pickup_day',\n",
    "        'HOUR(pickup_datetime)    AS pickup_hour',\n",
    "        'MINUTE(pickup_datetime)  AS pickup_min',\n",
    "        'SECOND(pickup_datetime)  AS pickup_sec',\n",
    "        'passenger_count', \n",
    "        'pickup_longitude', \n",
    "        'pickup_latitude', \n",
    "        'dropoff_longitude', \n",
    "        'dropoff_latitude',\n",
    "        'CASE WHEN store_and_fwd_flag == \"Y\" THEN 1 ELSE 0 END AS store_and_fwd_flag',\n",
    "        'trip_duration'\n",
    "    ).withColumn('distance_km', haversine_udf(col('pickup_longitude'), col('pickup_latitude'), col('dropoff_longitude'), col('dropoff_latitude')))\n",
    "\n",
    "extracted_test_df = casted_test_data.selectExpr(\n",
    "        'id',\n",
    "        'vendor_id',\n",
    "        'YEAR(pickup_datetime)    AS pickup_year',\n",
    "        'MONTH(pickup_datetime)   AS pickup_month',\n",
    "        'DAY(pickup_datetime)     AS pickup_day',\n",
    "        'HOUR(pickup_datetime)    AS pickup_hour',\n",
    "        'MINUTE(pickup_datetime)  AS pickup_min',\n",
    "        'SECOND(pickup_datetime)  AS pickup_sec',\n",
    "        'passenger_count', \n",
    "        'pickup_longitude', \n",
    "        'pickup_latitude', \n",
    "        'dropoff_longitude', \n",
    "        'dropoff_latitude',\n",
    "        'CASE WHEN store_and_fwd_flag == \"Y\" THEN 1 ELSE 0 END AS store_and_fwd_flag',\n",
    "    ).withColumn('distance_km', haversine_udf(col('pickup_longitude'), col('pickup_latitude'), col('dropoff_longitude'), col('dropoff_latitude')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for null values ​​in extracted_train_df:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 0 null value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vendor_id: 0 null value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_year: 0 null value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_month: 0 null value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_day: 0 null value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_hour: 0 null value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_min: 0 null value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_sec: 0 null value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passenger_count: 0 null value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_longitude: 0 null value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_latitude: 0 null value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropoff_longitude: 0 null value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropoff_latitude: 0 null value\n",
      "store_and_fwd_flag: 0 null value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_duration: 0 null value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance_km: 0 null value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Remove outliers\n",
    "extracted_train_df = extracted_train_df.filter(\n",
    "    (col('trip_duration') > 0) & \n",
    "    (col('trip_duration') < 36000) &  \n",
    "    (col('passenger_count') >= 1) & \n",
    "    (col('passenger_count') <= 6) & \n",
    "    (col('distance_km') > 0) & \n",
    "    (col('distance_km') < 100)  \n",
    ")\n",
    "\n",
    "# Check for null values\n",
    "print(\"Check for null values ​​in extracted_train_df:\")\n",
    "for column in extracted_train_df.columns:\n",
    "    null_count = extracted_train_df.filter(col(column).isNull()).count()\n",
    "    print(f\"{column}: {null_count} null value\")\n",
    "\n",
    "extracted_train_df = extracted_train_df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, convert the dataset into a RDD of `LabeledPoint` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature list (same as 3.2.1)\n",
    "feature_cols = ['vendor_id', 'pickup_year', 'pickup_month', 'pickup_day', 'pickup_hour',\n",
    "                'pickup_min', 'pickup_sec', 'passenger_count', 'pickup_longitude',\n",
    "                'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', \n",
    "                'store_and_fwd_flag', 'distance_km']\n",
    "\n",
    "def to_labeled_point(row):\n",
    "    features = [float(row[col]) for col in feature_cols]\n",
    "    # mapping vendor_id: 1 -> 0, 2 -> 1\n",
    "    features[0] = features[0] - 1  # vendor_id at index 0\n",
    "    return LabeledPoint(row['trip_duration'], features)\n",
    "\n",
    "train_data = extracted_train_df.rdd.map(to_labeled_point)\n",
    "\n",
    "# Testing set conversion\n",
    "def to_features_test(row):\n",
    "    features = [float(row[col]) for col in feature_cols]\n",
    "    features[0] = features[0] - 1  # mapping vendor_id: 1 -> 0, 2 -> 1\n",
    "    return features\n",
    "\n",
    "test_features = extracted_test_df.rdd.map(to_features_test)\n",
    "test_ids = extracted_test_df.rdd.map(lambda row: row['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[197] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "# Training set proportion parameter:\n",
    "train_size = 0.8\n",
    "\n",
    "train_rdd, validation_rdd = train_data.randomSplit([train_size, 1 - train_size], seed=24) #Fixed with seed for reproductivity\n",
    "\n",
    "# Cache RDD (Store train_rdd and validation_rdd in memory to reduce load)\n",
    "train_rdd.cache()\n",
    "validation_rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/12 10:03:56 WARN BlockManager: Task 86 already completed, not releasing lock for rdd_196_0\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use best parameters from 3.2.1\n",
    "best_params = {'maxDepth': 10}\n",
    "\n",
    "# Model training\n",
    "model = DecisionTree.trainRegressor(\n",
    "    train_rdd,\n",
    "    categoricalFeaturesInfo={0: 2, 7: 10, 12: 2},  # vendor_id, passenger_count, store_and_fwd_flag\n",
    "    impurity='variance',\n",
    "    maxDepth=best_params['maxDepth']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model evaluation (hold-out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatake/.local/lib/python3.10/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "[Stage 77:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on validation (MLlib RDD): 407.80706225247854\n",
      "R2 on validation (MLlib RDD): 0.4720785680004036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(validation_rdd.map(lambda lp: lp.features))\n",
    "labels_and_preds = validation_rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "metrics = RegressionMetrics(labels_and_preds)\n",
    "rmse = metrics.rootMeanSquaredError\n",
    "r2 = metrics.r2\n",
    "\n",
    "print(f\"RMSE on validation (MLlib RDD): {rmse}\")\n",
    "print(f\"R2 on validation (MLlib RDD): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison with Structured API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured API RMSE: 388.07311363218764\n",
      "Structured API R2: 0.6742563442546459\n",
      "RMSE Difference: 19.733948620290903\n",
      "R2 Difference: -0.20217777625424227\n"
     ]
    }
   ],
   "source": [
    "structured_api_rmse = 388.07311363218764 # Số từ 3.2.1\n",
    "structured_api_r2 =  0.6742563442546459  # Số từ 3.2.1\n",
    "print(f\"Structured API RMSE: {structured_api_rmse}\")\n",
    "print(f\"Structured API R2: {structured_api_r2}\")\n",
    "print(f\"RMSE Difference: {rmse - structured_api_rmse}\")\n",
    "print(f\"R2 Difference: {r2 - structured_api_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction (test file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|       id|     trip_duration|\n",
      "+---------+------------------+\n",
      "|id3004672|  811.665977443609|\n",
      "|id3505355| 734.6063756063757|\n",
      "|id1217141| 479.7964183139044|\n",
      "|id2150126|1195.0749776472092|\n",
      "|id1598245| 350.4942550386137|\n",
      "+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict(test_features)\n",
    "test_predictions_df = spark.createDataFrame(\n",
    "    test_ids.zip(test_predictions),\n",
    "    schema=['id', 'trip_duration']\n",
    ")\n",
    "\n",
    "# Write file\n",
    "test_predictions_df.coalesce(1).write.csv(\"prediction_mllib.csv\", header=True, mode='overwrite')\n",
    "\n",
    "test_predictions_df.show(5)\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MRJOB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
